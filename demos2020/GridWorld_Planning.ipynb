{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gym import spaces \n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "import IPython\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from rllib.environment.mdps import EasyGridWorld\n",
    "from rllib.policy import TabularPolicy\n",
    "from rllib.value_function import TabularQFunction, TabularValueFunction\n",
    "from rllib.util.neural_networks.utilities import one_hot_encode\n",
    "from matplotlib import rcParams\n",
    "import copy \n",
    "\n",
    "rcParams['font.size'] = 12\n",
    "\n",
    "rcParams['figure.figsize'] = (20, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def extract_policy(q_function):\n",
    "    \"\"\"Extract a policy from the q_function.\"\"\"\n",
    "    policy = TabularPolicy(num_states=q_function.num_states, num_actions=q_function.num_actions)\n",
    "    for state in range(policy.num_states):\n",
    "        q_val = q_function(torch.tensor(state).long())\n",
    "        action = torch.argmax(q_val)\n",
    "        \n",
    "        policy.set_value(state, action)\n",
    "    \n",
    "    return policy \n",
    "\n",
    "def plan_policy(state, environment, value_function, gamma):\n",
    "    next_value = torch.zeros(environment.num_actions)  # value of taking the different actions.\n",
    "    for action in range(environment.num_actions):\n",
    "        value_estimate = 0\n",
    "\n",
    "        # In practice, we do not have access to environment.transitions, but only to samples of it!.\n",
    "        for transition in environment.transitions[(state, action)]:  \n",
    "            next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "            reward = torch.tensor(transition[\"reward\"]).double()\n",
    "            value_estimate += transition[\"probability\"] * (\n",
    "                reward + gamma * value_function(next_state)\n",
    "            )\n",
    "            \n",
    "        next_value[action] = value_estimate\n",
    "    policy = torch.where(next_value == torch.max(next_value))[0]\n",
    "    \n",
    "    return policy \n",
    "\n",
    "def integrate_q(q_function, policy):\n",
    "    value_function = TabularValueFunction(num_states = q_function.num_states)\n",
    "    for state in range(policy.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        pi = Categorical(logits=policy(state))\n",
    "        value = 0\n",
    "        for action in range(policy.num_actions):\n",
    "            value += pi.probs[action] * q_function(state, torch.tensor(action).long())\n",
    "        \n",
    "        value_function.set_value(state, value)\n",
    "    \n",
    "    return value_function\n",
    "\n",
    "# Planning \n",
    "def init_value_function(num_states, terminal_states=None):\n",
    "    \"\"\"Initialize value function.\"\"\"\n",
    "    value_function = TabularValueFunction(num_states=num_states)\n",
    "    terminal_states = [] if terminal_states is None else terminal_states\n",
    "    for terminal_state in terminal_states:\n",
    "        value_function.set_value(terminal_state, 0)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "# Plotters\n",
    "def policy2str(policy):\n",
    "    if len(policy) == 4:\n",
    "        return \"*\"\n",
    "    \n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 in policy:\n",
    "        policy_str += down \n",
    "    if 1 in policy:\n",
    "        policy_str += up \n",
    "    if 2 in policy:\n",
    "        policy_str += right\n",
    "    if 3 in policy:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax.text(row, col, f\"{value_function[col, row]:.1f}\", ha=\"center\", va=\"center\", color=\"w\", fontsize=24)\n",
    "\n",
    "def plot_policy(policy, environment, ax):\n",
    "    ax.imshow(np.zeros((environment.height, environment.width)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            policy_ = policy(torch.tensor(row * environment.width + col))\n",
    "            ax.text(col, row, policy2str(policy_), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "\n",
    "def plot_induced_policy(environment, value_function, gamma, ax):\n",
    "    ax.imshow(np.zeros((environment.height, environment.width)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            policy_ = plan_policy(row * environment.width + col, environment, value_function, gamma)\n",
    "            ax.text(col, row, policy2str(policy_), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "            \n",
    "def plot_value_and_policy(value_function, policy):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 8))\n",
    "    \n",
    "    plot_value_function(value_function, axes[0]) \n",
    "\n",
    "environment = EasyGridWorld()\n",
    "Image(\"images/grid_world.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def policy_evaluation_step(environment, policy, value_function, gamma, gauss_seidel=False):\n",
    "    max_error = 0\n",
    "    avg_error = 0\n",
    "    if gauss_seidel:\n",
    "        old_value_function = value_function\n",
    "    else:\n",
    "        \n",
    "        old_value_function = copy.deepcopy(value_function)\n",
    "    for state in range(environment.num_states):\n",
    "        if state in environment.terminal_states:\n",
    "            continue\n",
    "\n",
    "        value_estimate = torch.tensor(0.0)\n",
    "        state = torch.tensor(state).long()\n",
    "        policy_ = Categorical(logits=policy(state))\n",
    "\n",
    "        for action in np.where(policy_.probs.detach().numpy())[0]:\n",
    "            p_action = policy_.probs[action].item()\n",
    "\n",
    "            # In RL, we do not have access to environment.transitions, but only to samples of it!.\n",
    "            for transition in environment.transitions[(state.item(), action)]:\n",
    "                next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                value_estimate += (\n",
    "                    p_action\n",
    "                    * transition[\"probability\"]\n",
    "                    * (transition[\"reward\"] + gamma * old_value_function(next_state).item())\n",
    "                )\n",
    "\n",
    "        value = value_function(state)\n",
    "        error = torch.abs(value_estimate - value).item()\n",
    "        max_error = max(max_error, error)\n",
    "        avg_error += error\n",
    "        value_function.set_value(state, value_estimate)\n",
    "    return max_error \n",
    "\n",
    "def iterative_policy_evaluation(environment, policy, gamma, eps=1e-6, max_iter=1000, gauss_seidel=False, value_function=None):\n",
    "    \"\"\"Implement Policy Evaluation algorithm (policy iteration without max).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    policy: TabularPolicy\n",
    "    gamma: float\n",
    "        discount factor.\n",
    "    eps: float \n",
    "        desired precision.\n",
    "    max_iter: int \n",
    "        Max number of iterations. \n",
    "    value_function: TabularValueFunction, optional. \n",
    "        Initial value function. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    value_function: TabularValueFunction\n",
    "        Value function associated to Policy.\n",
    "    num_iter: int\n",
    "        Number of iterations to reach `eps' accuracy. \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.1\n",
    "\n",
    "    \"\"\"\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "    for num_iter in range(max_iter):\n",
    "        max_error = policy_evaluation_step(environment, policy, value_function, gamma, gauss_seidel=gauss_seidel)\n",
    "\n",
    "        if max_error < eps:\n",
    "            break\n",
    "    return value_function, num_iter\n",
    "\n",
    "def pe_interact(gamma, noise, gauss_seidel):\n",
    "    environment = EasyGridWorld(noise=noise)\n",
    "    value_function = init_value_function(environment.num_states)\n",
    "    policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "\n",
    "    def plot_algorithm():\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        plot_policy(policy, environment, ax[1])\n",
    "        ax[1].set_title(\"Evaluated Policy\")\n",
    "        \n",
    "        plot_value_function(value_function.table.reshape(5, 5).detach().numpy(), ax[0])\n",
    "        ax[0].set_title(\"Value Function\")\n",
    "        \n",
    "        plt.show()\n",
    "        button = ipywidgets.Button(description=\"Step\")\n",
    "        button.on_click(lambda b: step())\n",
    "        display(button)\n",
    "        \n",
    "    def step():\n",
    "        max_error = policy_evaluation_step(environment, policy, value_function, gamma, gauss_seidel=gauss_seidel) \n",
    "        plot_algorithm()\n",
    "        print(f\"Max Bellman Error {max_error}\")\n",
    "        \n",
    "    plot_algorithm()\n",
    "    \n",
    "    \n",
    "interact(\n",
    "    pe_interact,\n",
    "    gamma=ipywidgets.FloatSlider(min=0, max=0.99, step=0.01, value=0.9, continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(min=0, max=0.9, step=0.01, value=0, continuous_update=False),\n",
    "    gauss_seidel=False\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def build_mrp_matrices(environment, policy):\n",
    "    mrp_kernel = np.zeros((environment.num_states, 1, environment.num_states))\n",
    "    mrp_reward = np.zeros((environment.num_states, 1))\n",
    "\n",
    "    for state in range(environment.num_states):\n",
    "        if state in environment.terminal_states:\n",
    "            mrp_kernel[state, 0, state] = 1\n",
    "            mrp_reward[state] = 0\n",
    "            continue\n",
    "\n",
    "        state = torch.tensor(state).long()\n",
    "        policy_ = Categorical(logits=policy(state))\n",
    "\n",
    "        for a, p_action in enumerate(policy_.probs):\n",
    "            for transition in environment.transitions[(state.item(), a)]:\n",
    "                with torch.no_grad():\n",
    "                    p_ns = transition[\"probability\"]\n",
    "                    mrp_reward[state, 0] += p_action * p_ns * transition[\"reward\"]\n",
    "                    mrp_kernel[state, 0, transition[\"next_state\"]] += p_action * p_ns\n",
    "    \n",
    "    return mrp_kernel, mrp_reward\n",
    "\n",
    "\n",
    "def average_policy_evaluation(environment, policy, value_function=None):\n",
    "    r\"\"\"Evaluate policy.\n",
    "\n",
    "    Finds stationary distribution (right eigenvector of 1 eigenvalue) and computes\n",
    "    ..math:: \\langle \\mu, r \\rangle.\n",
    "\n",
    "    \"\"\"\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "        \n",
    "    kernel, reward = build_mrp_matrices(environment=environment, policy=policy)\n",
    "    eig_values, eig_vectors = np.linalg.eig(kernel[:, 0, :].T)\n",
    "    idx = np.where(np.isclose(eig_values, 1))[0]\n",
    "    stationary_distirbution = eig_vectors[:, idx] / eig_vectors[:, idx].sum()\n",
    "    average_reward = torch.tensor(np.real(reward.T @ stationary_distirbution)).float()[0, 0].item()\n",
    "    \n",
    "    new_reward = reward[:, 0] - average_reward\n",
    "    V = np.zeros(environment.num_states)\n",
    "    for i in range(1000):\n",
    "        V = new_reward + kernel[:, 0] @ V\n",
    "        V -= V.mean()\n",
    "\n",
    "    for state in range(value_function.num_states):\n",
    "        value_function.set_value(state, V[state])\n",
    "    \n",
    "    return value_function \n",
    "\n",
    "def linear_system_policy_evaluation(environment, policy, gamma, value_function=None):\n",
    "    \"\"\"Evaluate a policy in an MDP solving the system bellman of equations.\n",
    "\n",
    "    V = r + gamma * P * V\n",
    "    V = (I - gamma * P)^-1 r\n",
    "    \"\"\"\n",
    "    if gamma == 1:\n",
    "        return average_policy_evaluation(environment, policy)\n",
    "    \n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "\n",
    "    kernel, reward = build_mrp_matrices(environment=environment, policy=policy)\n",
    "\n",
    "    A = torch.eye(environment.num_states) - gamma * kernel[:, 0, :]\n",
    "    # torch.testing.assert_allclose(A.inverse() @ A, torch.eye(model.num_states))\n",
    "    vals = A.inverse() @ reward[:, 0]\n",
    "    for state in range(environment.num_states):\n",
    "        value_function.set_value(state, vals[state].item())\n",
    "\n",
    "    return value_function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def value_iteration_step(environment, policy, value_function, gamma, gauss_seidel=False):\n",
    "    error = 0\n",
    "    if gauss_seidel:\n",
    "        old_value_function = value_function\n",
    "    else:\n",
    "        old_value_function = copy.deepcopy(value_function)\n",
    "    for state in range(environment.num_states):\n",
    "        value_ = torch.zeros(environment.num_actions)  # value of taking the different actions.\n",
    "        for action in range(environment.num_actions):\n",
    "            value_estimate = 0\n",
    "\n",
    "            # In practice, we do not have access to environment.transitions, but only to samples of it!.\n",
    "            for transition in environment.transitions[(state, action)]:  \n",
    "                next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                reward = torch.tensor(transition[\"reward\"]).double()\n",
    "                value_estimate += transition[\"probability\"] * (\n",
    "                    reward + gamma * old_value_function(next_state)\n",
    "                )\n",
    "            value_[action] = value_estimate\n",
    "        state = torch.tensor(state).long()\n",
    "        value = value_function(state)\n",
    "        value_, action = torch.max(value_, 0)\n",
    "\n",
    "        error = max(error, torch.abs(value_ - value.item()))\n",
    "        value_function.set_value(state, value_)\n",
    "        policy.set_value(state, action)\n",
    "    return error \n",
    "\n",
    "def value_iteration(environment, gamma, eps=1e-6, max_iter=1000, gauss_seidel=False, value_function=None, policy=None):\n",
    "    \"\"\"Implement of Value Iteration algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float\n",
    "        discount factor.\n",
    "    eps: float \n",
    "        desired precision.\n",
    "    max_iter: int \n",
    "        Max number of iterations. \n",
    "    value_function: TabularValueFunction, optional. \n",
    "        Initial value function. \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    value_function: TabularValueFunction\n",
    "        Optimal value function.\n",
    "    policy: Tabular Policy \n",
    "        Optimal policy. \n",
    "    num_iter: int\n",
    "        Number of iterations to reach `eps' accuracy. \n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.4\n",
    "\n",
    "    \"\"\"\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "    if policy is None:\n",
    "        policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "\n",
    "    for num_iter in range(max_iter):\n",
    "        error = value_iteration_step(environment, policy, value_function, gamma, gauss_seidel=gauss_seidel)\n",
    "\n",
    "        if error < eps:\n",
    "            break\n",
    "\n",
    "    return value_function, policy, num_iter \n",
    "\n",
    "\n",
    "\n",
    "def vi_interact(gamma, noise, gauss_seidel):\n",
    "    environment = EasyGridWorld(noise=noise)\n",
    "    value_function = init_value_function(environment.num_states)\n",
    "    policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "\n",
    "    def plot_algorithm():\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        plot_induced_policy(environment, value_function, gamma, ax[1])\n",
    "        ax[1].set_title(\"Induced Policy\")\n",
    "        \n",
    "        plot_value_function(value_function.table.reshape(5, 5).detach().numpy(), ax[0])\n",
    "        ax[0].set_title(\"Value Function\")\n",
    "        \n",
    "        plt.show()\n",
    "        button = ipywidgets.Button(description=\"Step\")\n",
    "        button.on_click(lambda b: step())\n",
    "        display(button)\n",
    "        \n",
    "    def step():\n",
    "        max_error = value_iteration_step(environment, policy, value_function, gamma, gauss_seidel=gauss_seidel) \n",
    "        plot_algorithm()\n",
    "        print(f\"Max Bellman Error {max_error}\")\n",
    "        \n",
    "    plot_algorithm()\n",
    "    \n",
    "    \n",
    "interact(\n",
    "    vi_interact,\n",
    "    gamma=ipywidgets.FloatSlider(min=0, max=0.99, step=0.01, value=0.9, continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(min=0, max=0.9, step=0.01, value=0, continuous_update=False),\n",
    "    gauss_seidel=False\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def policy_iteration_step(environment, policy, value_function, gamma):\n",
    "    value_function = linear_system_policy_evaluation(environment, policy, gamma)\n",
    "    policy_stable = True\n",
    "    for state in range(environment.num_states):\n",
    "\n",
    "        value_ = torch.zeros(environment.num_actions)\n",
    "        for action in range(environment.num_actions):\n",
    "            value_estimate = 0\n",
    "            for transition in environment.transitions[(state, action)]:\n",
    "                next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                reward = torch.tensor(transition[\"reward\"]).double()\n",
    "                value_estimate += transition[\"probability\"] * (\n",
    "                    reward + gamma * value_function(next_state).item()\n",
    "                )\n",
    "\n",
    "            value_[action] = value_estimate\n",
    "\n",
    "        state = torch.tensor(state).long()\n",
    "        old_policy = policy(state)\n",
    "        old_action = torch.argmax(old_policy)\n",
    "\n",
    "        action = torch.argmax(value_)\n",
    "        policy.set_value(state, action)\n",
    "\n",
    "        policy_stable &= (action == old_action).all().item()\n",
    "    \n",
    "    return value_function, policy_stable \n",
    "\n",
    "\n",
    "def policy_iteration(environment, gamma, max_iter=10, value_function=None, policy=None):\n",
    "    \"\"\"Implement Policy Iteration algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float.\n",
    "        discount factor.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.3\n",
    "\n",
    "    \"\"\"\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "    if policy is None:\n",
    "        policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "\n",
    "    for num_iter in range(max_iter):\n",
    "        # Evaluate the policy. \n",
    "        value_function, policy_stable = policy_iteration_step(environment, policy, value_function, gamma)\n",
    "        \n",
    "        if policy_stable:\n",
    "            break\n",
    "    return value_function, policy, num_iter\n",
    "\n",
    "\n",
    "def pi_interact(gamma, noise):\n",
    "    environment = EasyGridWorld(noise=noise)\n",
    "    value_function = init_value_function(environment.num_states)\n",
    "    policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "\n",
    "    def plot_algorithm(value_function):\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        fig, ax = plt.subplots(ncols=2)\n",
    "        plot_induced_policy(environment, value_function, gamma, ax[1])\n",
    "        ax[1].set_title(\"Induced Policy\")\n",
    "        \n",
    "        plot_value_function(value_function.table.reshape(5, 5).detach().numpy(), ax[0])\n",
    "        ax[0].set_title(\"Value Function\")\n",
    "        \n",
    "        plt.show()\n",
    "        button = ipywidgets.Button(description=\"Step\")\n",
    "        button.on_click(lambda b: step(value_function))\n",
    "        display(button)\n",
    "        \n",
    "    def step(value_function):        \n",
    "        value_function, policy_stable = policy_iteration_step(environment, policy, value_function, gamma) \n",
    "        plot_algorithm(value_function)\n",
    "        print(f\"Did the policy change? {'No' if policy_stable else 'Yes'}\")\n",
    "        \n",
    "    plot_algorithm(value_function)\n",
    "    \n",
    "    \n",
    "interact(\n",
    "    pi_interact,\n",
    "    gamma=ipywidgets.FloatSlider(min=0, max=0.99, step=0.01, value=0.9, continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(min=0, max=0.9, step=0.01, value=0, continuous_update=False)\n",
    ");\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def algorithm_interact(name, gamma, noise, eps, gauss_seidel):\n",
    "    environment = EasyGridWorld(noise=noise)\n",
    "    IPython.display.clear_output(wait=True)\n",
    "    plt.close()\n",
    "        \n",
    "    fig, ax = plt.subplots(ncols=2)\n",
    "\n",
    "    if name == \"Iterative Policy Evaluation\":\n",
    "        policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "        value_function, num_iter = iterative_policy_evaluation(environment, policy, gamma, max_iter=1000, eps=eps, gauss_seidel=gauss_seidel) \n",
    "        plot_policy(policy, environment, ax[1])\n",
    "        ax[1].set_title(\"Evaluated Policy\")\n",
    "\n",
    "    elif name == \"Linear System Policy Evaluation\":\n",
    "        policy = TabularPolicy(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "        value_function = linear_system_policy_evaluation(environment, policy, gamma)\n",
    "        num_iter = 1\n",
    "        plot_policy(policy, environment, ax[1])\n",
    "        ax[1].set_title(\"Evaluated Policy\")\n",
    "\n",
    "    elif name == \"Value Iteration\": \n",
    "        value_function, policy, num_iter = value_iteration(environment, gamma, eps=eps, gauss_seidel=gauss_seidel) \n",
    "        plot_induced_policy(environment, value_function, gamma, ax[1])\n",
    "        ax[1].set_title(\"Extracted Policy\")\n",
    "\n",
    "    elif name == \"Policy Iteration\": \n",
    "        value_function, policy, num_iter =  policy_iteration(environment, gamma)\n",
    "        plot_induced_policy(environment, value_function, gamma, ax[1])\n",
    "        ax[1].set_title(\"Extracted Policy\")\n",
    "\n",
    "    print(f\"Converged after {num_iter} {'iterations' if num_iter > 1 else 'iteration'}.\")\n",
    "\n",
    "    plot_value_function(value_function.table.reshape(5, 5).detach().numpy(), ax[0])\n",
    "    ax[0].set_title(\"Value Function\")\n",
    "    plt.show()\n",
    "    \n",
    "interact(\n",
    "    algorithm_interact,\n",
    "    name=[\"Value Iteration\", \"Policy Iteration\", \"Linear System Policy Evaluation\", \"Iterative Policy Evaluation\"],\n",
    "    gamma=ipywidgets.FloatSlider(min=0, max=0.99, step=0.01, value=0.9, continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(min=0, max=0.9, step=0.01, value=0, continuous_update=False),\n",
    "    eps=ipywidgets.FloatLogSlider(min=-6, max=-1, step=0.01, value=1e-3, continuous_update=False),\n",
    "    gauss_seidel=False\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
