{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from gym import spaces\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "import IPython\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 12\n",
    "rcParams['figure.figsize'] = (20, 8)\n",
    "\n",
    "from rllib.environment.mdps import EasyGridWorld\n",
    "from rllib.value_function import TabularQFunction, TabularValueFunction\n",
    "from rllib.util.neural_networks.utilities import one_hot_encode\n",
    "from rllib.policy import TabularPolicy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def extract_policy(q_function):\n",
    "    \"\"\"Extract a policy from the q_function.\"\"\"\n",
    "    policy = TabularPolicy(num_states=q_function.num_states,\n",
    "                           num_actions=q_function.num_actions)\n",
    "    for state in range(policy.num_states):\n",
    "        q_val = q_function(torch.tensor(state).long())\n",
    "        action = torch.argmax(q_val)\n",
    "\n",
    "        policy.set_value(state, action)\n",
    "\n",
    "    return policy\n",
    "\n",
    "\n",
    "def integrate_q(q_function, policy):\n",
    "    value_function = TabularValueFunction(num_states=q_function.num_states)\n",
    "    for state in range(policy.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        pi = Categorical(logits=policy(state))\n",
    "        value = 0\n",
    "        for action in range(policy.num_actions):\n",
    "            value += pi.probs[action] * \\\n",
    "                q_function(state, torch.tensor(action).long())\n",
    "\n",
    "        value_function.set_value(state, value)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "# Plotters\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax.text(j, i, f\"{value_function[i, j]:.1f}\",\n",
    "                    ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "def policy2str(policy):\n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 == policy:\n",
    "        policy_str += down \n",
    "    if 1 == policy:\n",
    "        policy_str += up \n",
    "    if 2 == policy:\n",
    "        policy_str += right\n",
    "    if 3 == policy:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax.text(row, col, f\"{value_function[col, row]:.1f}\", ha=\"center\", va=\"center\", color=\"w\", fontsize=24)\n",
    "\n",
    "def plot_policy(policy, ax):\n",
    "    rows, cols = policy.shape\n",
    "    ax.imshow(np.zeros((rows, cols)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            ax.text(col, row, policy2str(policy[row, col]), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "\n",
    "\n",
    "def plot_value_and_policy(value_function, policy):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 8))\n",
    "\n",
    "    plot_value_function(value_function, axes[0])\n",
    "    plot_policy(policy, axes[1])\n",
    "\n",
    "environment = EasyGridWorld()\n",
    "Image(\"images/grid_world.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exact Planning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "# Planning\n",
    "def init_value_function(num_states, terminal_states=None):\n",
    "    \"\"\"Initialize value function.\"\"\"\n",
    "    value_function = TabularValueFunction(num_states=num_states)\n",
    "    terminal_states = [] if terminal_states is None else terminal_states\n",
    "    for terminal_state in terminal_states:\n",
    "        value_function.set_value(terminal_state, 0)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def build_mrp_matrices(environment, policy):\n",
    "    mrp_kernel = np.zeros((environment.num_states, 1, environment.num_states))\n",
    "    mrp_reward = np.zeros((environment.num_states, 1))\n",
    "\n",
    "    for state in range(environment.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        policy_ = Categorical(logits=policy(state))\n",
    "\n",
    "        for a, p_action in enumerate(policy_.probs):\n",
    "            for transition in environment.transitions[(state.item(), a)]:\n",
    "                with torch.no_grad():\n",
    "                    p_ns = transition[\"probability\"]\n",
    "                    mrp_reward[state, 0] += p_action * p_ns * transition[\"reward\"]\n",
    "                    mrp_kernel[state, 0, transition[\"next_state\"]\n",
    "                               ] += p_action * p_ns\n",
    "\n",
    "    return mrp_kernel, mrp_reward\n",
    "\n",
    "\n",
    "def kernelreward2transitions(kernel, reward):\n",
    "    \"\"\"Transform a kernel and reward matrix into a transition dicitionary.\"\"\"\n",
    "    transitions = defaultdict(list)\n",
    "\n",
    "    num_states, num_actions = reward.shape\n",
    "\n",
    "    for state in range(num_states):\n",
    "        for action in range(num_actions):\n",
    "            for next_state in np.where(kernel[state, action])[0]:\n",
    "                transitions[(state, action)].append(\n",
    "                    {\n",
    "                        \"next_state\": next_state,\n",
    "                        \"probability\": kernel[state, action, next_state],\n",
    "                        \"reward\": reward[state, action],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    return transitions\n",
    "\n",
    "\n",
    "def linear_system_policy_evaluation(environment, policy, gamma, value_function=None):\n",
    "    \"\"\"Evaluate a policy in an MDP solving the system bellman of equations.\n",
    "\n",
    "    V = r + gamma * P * V\n",
    "    V = (I - gamma * P)^-1 r\n",
    "    \"\"\"\n",
    "\n",
    "    if value_function is None:\n",
    "        value_function = init_value_function(environment.num_states)\n",
    "\n",
    "    kernel, reward = build_mrp_matrices(environment=environment, policy=policy)\n",
    "\n",
    "    A = torch.eye(environment.num_states) - gamma * kernel[:, 0, :]\n",
    "    # torch.testing.assert_allclose(A.inverse() @ A, torch.eye(model.num_states))\n",
    "    vals = A.inverse() @ reward[:, 0]\n",
    "    for state in range(environment.num_states):\n",
    "        value_function.set_value(state, vals[state].item())\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "def policy_iteration(environment, gamma):\n",
    "    \"\"\"Implement Policy Iteration algorithm.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    gamma: float.\n",
    "        discount factor.\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction.\n",
    "    MIT press.\n",
    "    Chapter 4.3\n",
    "\n",
    "    \"\"\"\n",
    "    max_iter = 10\n",
    "    value_function = init_value_function(environment.num_states)\n",
    "    policy = TabularPolicy(num_states=environment.num_states,\n",
    "                           num_actions=environment.num_actions)\n",
    "\n",
    "    for num_iter in range(max_iter):\n",
    "        # Evaluate the policy.\n",
    "        value_function = linear_system_policy_evaluation(\n",
    "            environment, policy, gamma)\n",
    "        policy_stable = True\n",
    "        for state in range(environment.num_states):\n",
    "\n",
    "            value_ = torch.zeros(environment.num_actions)\n",
    "            for action in range(environment.num_actions):\n",
    "                value_estimate = 0\n",
    "                for transition in environment.transitions[(state, action)]:\n",
    "                    next_state = torch.tensor(transition[\"next_state\"]).long()\n",
    "                    reward = torch.tensor(transition[\"reward\"]).double()\n",
    "                    value_estimate += transition[\"probability\"] * (\n",
    "                        reward + gamma *\n",
    "                        value_function(next_state).item()\n",
    "                    )\n",
    "\n",
    "                value_[action] = value_estimate\n",
    "\n",
    "            state = torch.tensor(state).long()\n",
    "            old_policy = policy(state)\n",
    "            old_action = torch.argmax(old_policy)\n",
    "\n",
    "            action = torch.argmax(value_)\n",
    "            policy.set_value(state, action)\n",
    "\n",
    "            policy_stable &= (action == old_action).all().item()\n",
    "\n",
    "        if policy_stable:\n",
    "            break\n",
    "\n",
    "    return value_function, policy\n",
    "\n",
    "\n",
    "value_function, policy = policy_iteration(environment, 0.9)\n",
    "\n",
    "plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                      policy.table.argmax(0).reshape(5, 5).detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Based: Estimate a model, plan with such a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "    def __init__(self, num_states, num_actions, rmax):\n",
    "        self.num_states = num_states + 1\n",
    "        self.num_actions = num_actions \n",
    "        \n",
    "        self.probs = np.zeros((num_states + 1, num_actions, num_states + 1))\n",
    "        self.probs[:, :, -1] = 1\n",
    "        self.counts = np.zeros((num_states + 1, num_actions, num_states + 1))\n",
    "            \n",
    "        self.rewards = rmax * np.ones((num_states + 1, num_actions))         \n",
    "        self.transitions = kernelreward2transitions(self.probs, self.rewards)\n",
    "    \n",
    "    def update_model(self): \n",
    "        for state in range(self.num_states):\n",
    "            for action in range(self.num_actions):\n",
    "                n_sa = self.counts[state, action].sum()\n",
    "                if n_sa == 0:  # If state action pair was not visited, then do not update the model.\n",
    "                    continue\n",
    "                for next_state in range(self.num_states):\n",
    "                    self.probs[state, action, next_state] = self.counts[state, action, next_state] / n_sa\n",
    "        self.transitions = kernelreward2transitions(self.probs, self.rewards)\n",
    "        \n",
    "\n",
    "\n",
    "def rmax(gamma, noise, rmax_exploration):\n",
    "    global model, policy, value_function\n",
    "    environment = EasyGridWorld(noise=noise)\n",
    "    episode_horizon=20\n",
    "    if rmax_exploration:\n",
    "        rmax = 10\n",
    "    else:\n",
    "        rmax=0\n",
    "    model = Model(environment.num_states, environment.num_actions, rmax=rmax)\n",
    "    value_function, policy = policy_iteration(model, gamma=0.9)\n",
    "\n",
    "    def step(num_episodes):\n",
    "        global model, policy, value_function\n",
    "        for i_episode in range(num_episodes):\n",
    "            state = environment.reset()  # The initial distribution plays a big role. \n",
    "            for i in range(episode_horizon):\n",
    "                action = torch.argmax(policy(torch.tensor(state).long())).item()  # Rollout Policy  \n",
    "                next_state, reward, done, info = environment.step(action)\n",
    "\n",
    "                model.counts[state, action, next_state] += 1  # update count\n",
    "                model.rewards[state, action] = reward  # update model.\n",
    "\n",
    "                state = next_state \n",
    "\n",
    "            model.update_model()\n",
    "            value_function, policy = policy_iteration(model, gamma=0.9)\n",
    "        \n",
    "        plot()\n",
    "    \n",
    "    def plot():\n",
    "        plot_value_and_policy(\n",
    "            value_function.table[:, :-1].reshape(5, 5).detach().numpy(), \n",
    "            policy.table.argmax(0)[:-1].reshape(5, 5).detach().numpy()\n",
    "        )\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        button = ipywidgets.Button(description=\"Step 1 Episode\")\n",
    "        button.on_click(lambda b: step(num_episodes=1))\n",
    "        display(button)\n",
    "\n",
    "        button2 = ipywidgets.Button(description=\"Step 10 Episodes\")\n",
    "        button2.on_click(lambda b: step(num_episodes=10))\n",
    "        display(button2)\n",
    "        plt.show()\n",
    "        print(\"rewards [state x actions]\")\n",
    "        print(model.rewards)\n",
    "\n",
    "        \n",
    "    plot()\n",
    "        \n",
    "        \n",
    "interact(\n",
    "    rmax,\n",
    "    gamma=ipywidgets.FloatSlider(\n",
    "        value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(\n",
    "        value=0, min=0., max=0.2, step=1e-2, continuous_update=False),\n",
    "    rmax_exploration=False,\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
