{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.distributions import Categorical\n",
    "import matplotlib.pyplot as plt \n",
    "from matplotlib import rcParams\n",
    "rcParams['font.size'] = 24\n",
    "rcParams['figure.figsize'] = (16, 8)\n",
    "from tqdm import tqdm \n",
    "\n",
    "import importlib \n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "import IPython\n",
    "\n",
    "from rllib.dataset.datatypes import Observation\n",
    "from rllib.util.utilities import get_entropy_and_log_p\n",
    "\n",
    "from rllib.util.training.agent_training import train_agent\n",
    "from rllib.environment import GymEnvironment\n",
    "from rllib.environment.mdps import EasyGridWorld\n",
    "from rllib.policy import TabularPolicy\n",
    "from rllib.value_function import TabularQFunction, TabularValueFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def extract_policy(q_function):\n",
    "    \"\"\"Extract a policy from the q_function.\"\"\"\n",
    "    policy = TabularPolicy(num_states=q_function.num_states,\n",
    "                           num_actions=q_function.num_actions)\n",
    "    for state in range(policy.num_states):\n",
    "        q_val = q_function(torch.tensor(state).long())\n",
    "        action = torch.argmax(q_val)\n",
    "\n",
    "        policy.set_value(state, action)\n",
    "\n",
    "    return policy\n",
    "\n",
    "def integrate_q(q_function, policy):\n",
    "    value_function = TabularValueFunction(num_states=q_function.num_states)\n",
    "    for state in range(policy.num_states):\n",
    "        state = torch.tensor(state).long()\n",
    "        pi = Categorical(logits=policy(state))\n",
    "        value = 0\n",
    "        for action in range(policy.num_actions):\n",
    "            value += pi.probs[action] * \\\n",
    "                q_function(state, torch.tensor(action).long())\n",
    "\n",
    "        value_function.set_value(state, value)\n",
    "\n",
    "    return value_function\n",
    "\n",
    "\n",
    "environment = EasyGridWorld()\n",
    "Image(\"images/grid_world.png\")\n",
    "\n",
    "# Plotters\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for i in range(rows):\n",
    "        for j in range(cols):\n",
    "            ax.text(j, i, f\"{value_function[i, j]:.1f}\",\n",
    "                    ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "def policy2str(policy):\n",
    "    left = u'\\u2190'\n",
    "    right = u'\\u2192'\n",
    "    up = u'\\u2191'\n",
    "    down = u'\\u2193'\n",
    "    policy_str = \"\"\n",
    "    if 0 == policy:\n",
    "        policy_str += down \n",
    "    if 1 == policy:\n",
    "        policy_str += up \n",
    "    if 2 == policy:\n",
    "        policy_str += right\n",
    "    if 3 == policy:\n",
    "        policy_str += left\n",
    "    return policy_str\n",
    "\n",
    "def plot_value_function(value_function, ax):\n",
    "    ax.imshow(value_function)\n",
    "    rows, cols = value_function.shape\n",
    "    for row in range(rows):\n",
    "        for col in range(cols):\n",
    "            ax.text(row, col, f\"{value_function[col, row]:.1f}\", ha=\"center\", va=\"center\", color=\"w\", fontsize=24)\n",
    "\n",
    "def plot_policy(policy, ax):\n",
    "    rows, cols = policy.shape\n",
    "    ax.imshow(np.zeros((rows, cols)))\n",
    "    for row in range(environment.height):\n",
    "        for col in range(environment.width):\n",
    "            ax.text(col, row, policy2str(policy[row, col]), ha=\"center\", va=\"center\", color=\"r\", fontsize=24)\n",
    "\n",
    "\n",
    "def plot_value_and_policy(value_function, policy):\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(20, 8))\n",
    "\n",
    "    plot_value_function(value_function, axes[0])\n",
    "    plot_policy(policy, axes[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Q Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "def q_learning(gamma=0.9, alpha=0.5, eps=0., optimistic_init=False):\n",
    "    global state \n",
    "    q_function = TabularQFunction(\n",
    "        num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "    nn.init.ones_(q_function.nn.head.weight)\n",
    "\n",
    "    if optimistic_init:\n",
    "        q_function.nn.head.weight.data = 10 / \\\n",
    "            (1 - gamma) * q_function.nn.head.weight.data  # Initialization\n",
    "\n",
    "    state = environment.reset()\n",
    "\n",
    "    def step(num_iter):\n",
    "        global state\n",
    "\n",
    "        for i in range(num_iter):\n",
    "            if np.random.rand() < eps:\n",
    "                action = np.random.choice(environment.num_actions)\n",
    "            else:\n",
    "                action = torch.argmax(q_function(\n",
    "                    torch.tensor(state).long())).item()\n",
    "\n",
    "            q_val = q_function(torch.tensor(state).long(),\n",
    "                               torch.tensor(action).long())\n",
    "\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "\n",
    "            next_q = torch.max(q_function(torch.tensor(next_state).long()))\n",
    "            reward = torch.tensor(reward).double()\n",
    "            td = reward + gamma * next_q - q_val\n",
    "\n",
    "            q_function.set_value(state, action, q_val + alpha * td)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        plot()\n",
    "\n",
    "    def plot():\n",
    "        policy = extract_policy(q_function)\n",
    "        value_function = integrate_q(q_function, policy)\n",
    "\n",
    "        plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(),\n",
    "                              policy.table.argmax(0).reshape(5, 5).detach().numpy())\n",
    "        \n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(plt.gcf())\n",
    "        plt.close()\n",
    "\n",
    "        button = ipywidgets.Button(description=\"Step 100\")\n",
    "        button.on_click(lambda b: step(num_iter=100))\n",
    "        display(button)\n",
    "\n",
    "        button2 = ipywidgets.Button(description=\"Step 1000\")\n",
    "        button2.on_click(lambda b: step(num_iter=1000))\n",
    "        display(button2)\n",
    "    plot()\n",
    "\n",
    "\n",
    "interact(\n",
    "    q_learning,\n",
    "    gamma=ipywidgets.FloatSlider(\n",
    "        value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    alpha=ipywidgets.FloatSlider(\n",
    "        value=0.5, min=0., max=2.0, step=1e-2, continuous_update=False),\n",
    "    eps=ipywidgets.FloatSlider(\n",
    "        value=0., min=0., max=1.0, step=1e-2, continuous_update=False),\n",
    "    optimistic_init=ipywidgets.Checkbox(value=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning with function approximation\n",
    "\n",
    "- Q Learning: approximate Q with a parametric function. \n",
    "- DQN: Approximate Q with a parametric function and use a target network to compute the delays. \n",
    "https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf\n",
    "- DDQN: Approximate Q with a parametric function and use the target network to compute the maximum. See https://arxiv.org/pdf/1509.06461.pdf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {},
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from rllib.policy import EpsGreedy, SoftMax\n",
    "from rllib.util.parameter_decay import ExponentialDecay\n",
    "\n",
    "def run(env_name, agent_name, exploration):\n",
    "    if \"CartPole\" in env_name:\n",
    "        max_steps = 200\n",
    "    else:\n",
    "        max_steps = 1000 \n",
    "        \n",
    "    environment = GymEnvironment(env_name)\n",
    "    agent = getattr(\n",
    "        importlib.import_module(\"rllib.agent\"), \n",
    "        f\"{agent_name}Agent\"\n",
    "    ).default(environment)\n",
    "    \n",
    "    if exploration == \"eps-greedy\":\n",
    "        policy = EpsGreedy(agent.algorithm.critic, ExponentialDecay(start=1.0, end=0.01, decay=500))\n",
    "    elif exploration == \"softmax\":\n",
    "        policy = SoftMax(agent.algorithm.critic, ExponentialDecay(start=1.0, end=0.01, decay=500))\n",
    "    agent.set_policy(policy)\n",
    "    try:\n",
    "        train_agent(environment=environment, agent=agent, num_episodes=50, max_steps=max_steps, render=True, plot_flag=False)\n",
    "    except KeyboardInterrupt:\n",
    "        pass \n",
    "    environment.close()\n",
    "    \n",
    "    IPython.display.clear_output()\n",
    "    \n",
    "    plt.plot(agent.logger.get(\"train_return-0\"), linewidth=16)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Return\");\n",
    "    \n",
    "interact(\n",
    "    run,\n",
    "    env_name = [\"CartPole-v0\", \"Acrobot-v1\", \"MountainCar-v0\"],\n",
    "    agent_name = [\"QLearning\", \"DQN\", \"DDQN\"],\n",
    "    exploration = [\"softmax\", \"eps-greedy\"]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Tabular SARSA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": [
    "environment = EasyGridWorld()\n",
    "def sarsa(gamma=0.9, alpha=0.5, eps=0., optimistic_init=False):\n",
    "    global state, action \n",
    "    print(environment.num_states, environment.num_actions)\n",
    "    q_function = TabularQFunction(num_states=environment.num_states, num_actions=environment.num_actions)\n",
    "    nn.init.ones_(q_function.nn.head.weight)\n",
    "    if optimistic_init:\n",
    "        q_function.nn.head.weight.data = 10 / (1 - gamma) * q_function.nn.head.weight.data\n",
    "        \n",
    "    state = environment.reset()\n",
    "    if np.random.rand() < eps:\n",
    "        action = np.random.choice(environment.num_actions)\n",
    "    else:\n",
    "        action = torch.argmax(q_function(torch.tensor(state).long())).item()\n",
    "\n",
    "    def step(num_iter):\n",
    "        global state, action\n",
    "        for i in range(num_iter):\n",
    "            q_val = q_function(torch.tensor(state).long(), torch.tensor(action).long())\n",
    "\n",
    "            next_state, reward, done, info = environment.step(action)\n",
    "    \n",
    "            if np.random.rand() < eps:\n",
    "                next_action = np.random.choice(environment.num_actions)\n",
    "            else:\n",
    "                next_action = torch.argmax(q_function(torch.tensor(next_state).long())).item()\n",
    "\n",
    "\n",
    "            next_q = q_function(torch.tensor(next_state).long(), torch.tensor(next_action).long())\n",
    "            reward = torch.tensor(reward).double()\n",
    "            td = reward + gamma * next_q - q_val \n",
    "\n",
    "            q_function.set_value(state, action, q_val + alpha * td)\n",
    "            state, action = next_state, next_action\n",
    "\n",
    "        plot()\n",
    "              \n",
    "    \n",
    "    def plot():\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        \n",
    "        policy = extract_policy(q_function)\n",
    "        value_function = integrate_q(q_function, policy)\n",
    "\n",
    "\n",
    "        plot_value_and_policy(value_function.table.reshape(5, 5).detach().numpy(), \n",
    "                              policy.table.argmax(0).reshape(5, 5).detach().numpy())\n",
    "        \n",
    "        \n",
    "        button = ipywidgets.Button(description=\"Step 100\")\n",
    "        button.on_click(lambda b: step(num_iter=100))\n",
    "        display(button)\n",
    "        \n",
    "        button2 = ipywidgets.Button(description=\"Step 1000\")\n",
    "        button2.on_click(lambda b: step(num_iter=1000))\n",
    "        display(button2)\n",
    "    plot()\n",
    "\n",
    "interact(\n",
    "    sarsa, \n",
    "    gamma=ipywidgets.FloatSlider(value=0.9, min=0., max=0.99, step=1e-2, continuous_update=False),\n",
    "    alpha=ipywidgets.FloatSlider(value=0.5, min=0., max=2.0, step=1e-2, continuous_update=False),\n",
    "    eps=ipywidgets.FloatSlider(value=0., min=0., max=1.0, step=1e-2, continuous_update=False),\n",
    "    optimistic_init=ipywidgets.Checkbox(value=False)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {}
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
