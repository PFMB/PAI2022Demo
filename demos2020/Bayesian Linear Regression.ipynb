{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T10:36:44.080900Z",
     "start_time": "2021-10-08T10:36:34.831931Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "import IPython\n",
    "# If in your browser the figures are not nicely vizualized, change the following line.\n",
    "rcParams['font.size'] = 12\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "layout = ipywidgets.Layout(width='auto', height='40px')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Linear Regression:\n",
    "\n",
    "Consider a data generating process given by the following probabilistic model, $y |w, x \\sim \\mathcal{N}(w^\\top x, \\sigma_n^2)$. The objective is to find the posterior distribution of $p(w|X,Y)$. \n",
    "\n",
    "Using Bayes' Rule, $ p(w|X,y) = \\frac{p(y|w,X) p(w|X) }{p(y|X)}$, where $p(y|w,X)$ is the likelihood, $p(w|X)$ is the prior, and $p(y|X)$ is a normalization factor. \n",
    "\n",
    "In our case, $p(y|w,X) = \\mathcal{N}(w^\\top x, \\sigma_n^2)$. Compared to ordinary linear regression, with Bayesian Regression we have the freedom to place a prior $p(w|X)$. In this example, we will use a gaussian prior on $w$ given by $p(w|X) = \\mathcal{N}(0, \\Sigma_p^2)$. In this particular case, we have a closed form of the posterior, that is given by: $$p(y|w,X) = \\mathcal{N}(\\sigma_n^{-2} A^{-1} X^\\top y, A^{-1}),$$\n",
    "where $A = \\sigma_n^{-2} X^\\top  X+ \\Sigma_p^{-1}$. \n",
    "\n",
    "\n",
    "### Maximum Likelihood estimator: \n",
    "The maximum likelihood estimator is a point estimate $\\hat{w}_\\text{MLE}$ that maximizes the likelihood of the observations, i.e., $\\hat{w}_\\text{MLE} = \\arg \\max p(y|w, X)$. For this particular model, this is given by $\\hat{w}_\\text{MLE} = (X^\\top  X)^{-1} X^\\top y$. This coincides with the Ordinary Least Squares solution.\n",
    "\n",
    "\n",
    "### Maximum a Posteriori estimator: \n",
    "The maximum-a-posteriori estimator, instead, maximizes the posterior probability, i.e.,  $\\hat{w}_\\text{MAP} = \\arg \\max p(w|y, X) = \\arg \\max p(y|w, X) p(w|X)$. For this particular model, this is given by $\\hat{w}_\\text{MAP} = (X^\\top  X + \\sigma_n^2 \\Sigma_p^{-1})^{-1} X^\\top y$, i.e. Ridge Linear Regression with regularization $\\lambda = \\sigma_n^2 \\Sigma_p^{-1}$. \n",
    "\n",
    "### Predictive Distribution:\n",
    "The predictive distribution at test locations $X_\\star$ is $p(y_\\star|X_\\star, X, y) = \\mathcal{N}(\\sigma_n^{-2} X_\\star A^{-1} X^\\top y, X_\\star A^{-1} X_\\star^\\top) $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T16:00:28.375843Z",
     "start_time": "2021-10-08T16:00:27.176814Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 12)\n",
    "rcParams['font.size'] = 16\n",
    "rcParams['lines.linewidth'] = 4\n",
    "rcParams['lines.markersize'] = 20\n",
    "colors = sns.color_palette()\n",
    "true_c, mle_c, map_c = colors[0], colors[2], colors[3]\n",
    "prior_color, posterior_color = colors[5], colors[6]\n",
    "epistemic_color, aleatoric_color = colors[1], colors[4]\n",
    "\n",
    "w = np.random.multivariate_normal(np.zeros(2), np.array([[0.5, 0], [0, 1.]]))\n",
    "\n",
    "def blr(num_points, noise, x0):\n",
    "    num_test_points = 1000\n",
    "    X = np.stack((np.ones(num_points), np.random.rand(num_points))).T\n",
    "    X_plot = np.stack(\n",
    "        (np.ones(num_test_points), np.linspace(-0.5, 1.5, num_test_points))).T\n",
    "    sigma_p = np.array([[1., 0], [0, 1.]])  # Covariance matrix of posterior.\n",
    "\n",
    "    # Sample a parameter from the prior.\n",
    "    def get_w():\n",
    "        return np.random.multivariate_normal(\n",
    "            np.zeros(2), np.array([[0.5, 0], [0, 1.]]))\n",
    "    def sample_w():\n",
    "        global w\n",
    "        w = get_w()\n",
    "        sample()\n",
    "\n",
    "    def sample():\n",
    "        y = X @ w + noise * np.random.randn(num_points)\n",
    "\n",
    "        # Calculate the posterior mean and covariance\n",
    "        A = 1 / (noise ** 2) * X.T @ X + np.linalg.pinv(sigma_p)\n",
    "        A_inv = np.linalg.pinv(A)\n",
    "        mu_posterior = 1 / (noise ** 2) * A_inv @ X.T @ y\n",
    "        cov_posterior = A_inv\n",
    "\n",
    "        # MLE solution.\n",
    "        w_MLE = np.linalg.pinv(X.T @ X) @ X.T @ y\n",
    "        y_MLE = X_plot @ w_MLE\n",
    "\n",
    "        # MAP Solution.\n",
    "        w_MAP = mu_posterior\n",
    "        y_MAP = X_plot @ w_MAP\n",
    "\n",
    "        # Predictive mean and variance\n",
    "        mu_pred = X_plot @ mu_posterior\n",
    "        std_dev = np.diagonal(X_plot @ A_inv @ X_plot.T) ** 0.5\n",
    "\n",
    "        fig, axes = plt.subplots(2, 2)\n",
    "\n",
    "        # Predictions.\n",
    "        axes[0, 0].plot(X[:, 1], y, '*', color=true_c, label=\"True Data\")\n",
    "        axes[0, 0].plot(X_plot[:, 1], y_MLE, '--', color=mle_c, label='MLE')\n",
    "        axes[0, 0].plot(X_plot[:, 1], y_MAP, '--', color=map_c, label='MAP')\n",
    "        axes[0, 0].plot(X_plot[:, 1], X_plot @ w, '--', color=true_c, label='True')\n",
    "\n",
    "        axes[0, 0].fill_between(X_plot[:, 1], mu_pred - 3 * std_dev, mu_pred +\n",
    "                                3 * std_dev, color=epistemic_color, alpha=0.5, label='Epistemic Uncertainty')\n",
    "        axes[0, 0].fill_between(X_plot[:, 1], mu_pred - 3 * std_dev, mu_pred - 3 * std_dev - 3 * noise, \n",
    "                                color=aleatoric_color, alpha=0.5, label='Aleatoric Uncertainty')\n",
    "        axes[0, 0].fill_between(X_plot[:, 1], mu_pred + 3 * std_dev, mu_pred + 3 * std_dev + 3 * noise, \n",
    "                                color=aleatoric_color, alpha=0.5)\n",
    "\n",
    "        w_sample = np.random.multivariate_normal(mu_posterior, cov_posterior)\n",
    "        axes[0, 0].plot(X_plot[:, 1], X_plot @ w_sample, '-.', color=posterior_color, alpha=0.5, label='Posterior Samples')\n",
    "        for k in range(3):\n",
    "            w_sample = np.random.multivariate_normal(mu_posterior, cov_posterior)\n",
    "            axes[0, 0].plot(X_plot[:, 1], X_plot @ w_sample, '-.', alpha=0.5, color=posterior_color)\n",
    "\n",
    "        axes[0, 0].legend(loc='lower left', ncol=2)\n",
    "        axes[0, 0].set_xlabel('X')\n",
    "        axes[0, 0].set_ylabel('y')\n",
    "        axes[0, 0].set_title('Predictions')\n",
    "\n",
    "        # Posterior distribution at X = 1\n",
    "        x_star = np.array([1., x0])\n",
    "\n",
    "        y_mle = x_star @ w_MLE\n",
    "        y_map = x_star @ w_MAP\n",
    "\n",
    "        mu = x_star @ mu_posterior\n",
    "        sigma = (x_star @ A_inv @ x_star.T) ** 0.5\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        axes[0, 1].plot(x, stats.norm.pdf(x, mu, sigma),\n",
    "                        color=map_c, label='Predictive Distribution')\n",
    "        axes[0, 1].vlines(y_mle, 0, 5, color=mle_c, linestyle='dashed', label='MLE')\n",
    "        axes[0, 1].vlines(y_map, 0, 5, color=map_c, linestyle='dashed', label='MAP')\n",
    "        axes[0, 1].vlines(x_star @ w, 0, 5, color=true_c, linestyle='dashed', label='True')\n",
    "        axes[0, 1].legend(loc='upper left')\n",
    "        axes[0, 1].set_title(f'Predictive distribtuion at X={x0}')\n",
    "\n",
    "        # Posterior distribution of w_0\n",
    "        mu = mu_posterior[0]\n",
    "        sigma = cov_posterior[0, 0] ** 0.5\n",
    "\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        axes[1, 0].plot(x, stats.norm.pdf(x, mu, sigma), color=map_c, label='Posterior')\n",
    "\n",
    "        x = np.linspace(- 3 * sigma_p[0, 0], 3 * sigma_p[0, 0], 100)\n",
    "        axes[1, 0].plot(x, stats.norm.pdf(x, np.zeros(\n",
    "            100), sigma_p[0, 0]), color=prior_color, label='Prior')\n",
    "\n",
    "        axes[1, 0].vlines(w_MLE[0], 0, 5, color=mle_c, linestyle='dashed', label='MLE')\n",
    "        axes[1, 0].vlines(w_MAP[0], 0, 5, color=map_c, linestyle='dashed', label='MAP')\n",
    "        axes[1, 0].vlines(w[0], 0, 5, color=true_c, linestyle='dashed', label='True')\n",
    "\n",
    "        axes[1, 0].legend(loc='upper left')\n",
    "        axes[1, 0].set_title('Posterior Distribution of Bias')\n",
    "\n",
    "        # Posterior distribution of w_1\n",
    "        mu = mu_posterior[1]\n",
    "        sigma = cov_posterior[1, 1] ** 0.5\n",
    "\n",
    "        x = np.linspace(mu - 3*sigma, mu + 3*sigma, 100)\n",
    "        axes[1, 1].plot(x, stats.norm.pdf(x, mu, sigma),\n",
    "                        color=map_c, label='Posterior')\n",
    "\n",
    "        x = np.linspace(- 3 * sigma_p[1, 1], 3 * sigma_p[1, 1], 100)\n",
    "        axes[1, 1].plot(x, stats.norm.pdf(x, np.zeros(\n",
    "            100), sigma_p[1, 1]), color=prior_color, label='Prior')\n",
    "\n",
    "        axes[1, 1].vlines(w_MLE[1], 0, 5, color=mle_c, linestyle='dashed', label='MLE')\n",
    "        axes[1, 1].vlines(w_MAP[1], 0, 5, color=map_c, linestyle='dashed', label='MAP')\n",
    "        axes[1, 1].vlines(w[1], 0, 5, color=true_c, linestyle='dashed', label='True')\n",
    "        axes[1, 1].set_title('Posterior Distribution of Weight')\n",
    "\n",
    "        axes[0, 0].set_xlim([-0.5, 1.5])\n",
    "        axes[0, 0].set_ylim([-5, 5])\n",
    "\n",
    "        axes[0, 1].set_xlim([-3, 3])\n",
    "        axes[0, 1].set_ylim([0, 3])\n",
    "        \n",
    "        axes[1, 0].set_xlim([-3, 3])\n",
    "        axes[1, 0].set_ylim([0, 2])\n",
    "        \n",
    "        axes[1, 1].set_xlim([-3, 3])\n",
    "        axes[1, 1].set_ylim([0, 2])\n",
    "        \n",
    "        plt.tight_layout(pad=0.2)\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(plt.gcf())\n",
    "        plt.close()\n",
    "\n",
    "        button = ipywidgets.Button(\n",
    "            description=\"Re-Sample y (from likelihood)\", layout=layout)\n",
    "        button.on_click(lambda b: sample())\n",
    "        display(button)\n",
    "        button_w = ipywidgets.Button(\n",
    "            description=\"Re-Sample w (from prior)\", layout=layout)\n",
    "        button_w.on_click(lambda b: sample_w())\n",
    "        display(button_w)\n",
    "    sample()\n",
    "\n",
    "interact(blr,\n",
    "         num_points=ipywidgets.IntSlider(\n",
    "             value=5,\n",
    "             min=3,\n",
    "             max=200,\n",
    "             description='Number of Data Points:',\n",
    "             style={'description_width': 'initial'},\n",
    "             continuous_update=False\n",
    "         ),\n",
    "         noise=ipywidgets.FloatSlider(\n",
    "             value=0.5,\n",
    "             min=0,\n",
    "             max=2,\n",
    "             step=0.1,\n",
    "             description='Likelihood std dev:',\n",
    "             style={'description_width': 'initial'},\n",
    "             continuous_update=False),\n",
    "         x0=ipywidgets.FloatSlider(\n",
    "             value=0.5,\n",
    "             min=-0.5,\n",
    "             max=1.5,\n",
    "             step=0.1,\n",
    "             description='Test point:',\n",
    "             style={'description_width': 'initial'},\n",
    "             continuous_update=False)\n",
    "         );"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Linear Regression on Real Data: Predict the Bike demand in Winter Working Sunny Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:11:37.284124Z",
     "start_time": "2021-10-08T15:11:37.079769Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (10, 5)\n",
    "df = pd.read_csv(\"../data/bike-sharing.csv\")\n",
    "Tmin, Tmax =-16, +50 \n",
    "df_filtered = df[(df.season == 1)  & (df.workingday == 0)  & (df.weathersit == 1)]\n",
    "\n",
    "# y = df_filtered[\"count\"].values\n",
    "y = df_filtered[\"cnt\"].values # Normalize\n",
    "mean, std = np.mean(y), np.std(y)\n",
    "y = (y - np.mean(y)) / std\n",
    "X_ = df_filtered[[\"atemp\"]].values\n",
    "\n",
    "plt.plot(Tmin + (Tmax - Tmin) * X_, std * y + mean, '*');\n",
    "plt.xlabel('Temperature')\n",
    "plt.ylabel('Bike Count')\n",
    "plt.title(\"Bike Usage\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-08T15:13:36.143048Z",
     "start_time": "2021-10-08T15:13:35.698980Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (20, 8)\n",
    "rcParams['font.size'] = 16\n",
    "rcParams['lines.linewidth'] = 5\n",
    "rcParams['lines.markersize'] = 16\n",
    "\n",
    "def bike_prediction(noise, prior):\n",
    "    N = len(y)\n",
    "    X_plot = np.atleast_2d(np.linspace(0, 1.1 * np.max(X_), 100)).T\n",
    "\n",
    "    X = np.concatenate((np.ones((N, 1)), X_), axis=1)  # Attach a bias.\n",
    "    X_plot = np.concatenate((np.ones((100, 1)), X_plot), axis=1)  # Attach a bias.\n",
    "    sigma_p = prior * np.eye(2)\n",
    "\n",
    "    # # Calculate the posterior mean and covariance\n",
    "\n",
    "    A = 1 / (noise ** 2) * X.T @ X + np.linalg.inv(sigma_p) \n",
    "    A_inv = np.linalg.inv(A)\n",
    "    mu_posterior = 1 / (noise ** 2) * A_inv @ X.T @ y \n",
    "    cov_posterior = A_inv\n",
    "\n",
    "    # # MLE solution.\n",
    "    w_MLE = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    y_MLE = X_plot @ w_MLE\n",
    "\n",
    "    # # MAP Solution.\n",
    "    w_MAP = mu_posterior \n",
    "    y_MAP = X_plot @ w_MAP\n",
    "\n",
    "    # Predictive mean and variance\n",
    "    mu_pred = X_plot @ mu_posterior\n",
    "    std_dev = np.diagonal(X_plot @ A_inv @ X_plot.T) ** 0.5\n",
    "    \n",
    "    # Estimate aleatoric uncertainty:\n",
    "\n",
    "    plt.plot(Tmin + (Tmax - Tmin) * X[:, 1], mean + std * y, 'b*', label='train data')\n",
    "    plt.plot(Tmin + (Tmax - Tmin) * X_plot[:, 1], mean + std * y_MLE, 'r--', label='MLE')\n",
    "    plt.plot(Tmin + (Tmax - Tmin) * X_plot[:, 1], mean + std * y_MAP, 'g--', label='MAP')\n",
    "    plt.fill_between(\n",
    "        Tmin + (Tmax - Tmin) * X_plot[:, 1], \n",
    "        mean + std * (mu_pred - 3 * std_dev), \n",
    "        mean + std * (mu_pred + 3 * std_dev), \n",
    "        color=epistemic_color, \n",
    "        alpha=0.5, \n",
    "        label='Epistemic Uncertainty'\n",
    "    )\n",
    "    \n",
    "    plt.fill_between(\n",
    "        Tmin + (Tmax - Tmin) * X_plot[:, 1], \n",
    "        mean + std * (mu_pred - 3 * std_dev - 3 * noise), \n",
    "        mean + std * (mu_pred - 3 * std_dev), \n",
    "        color=aleatoric_color, \n",
    "        alpha=0.5, \n",
    "        label='Epistemic Uncertainty'\n",
    "    )\n",
    "    \n",
    "    plt.fill_between(\n",
    "        Tmin + (Tmax - Tmin) * X_plot[:, 1], \n",
    "        mean + std * (mu_pred + 3 * std_dev + 3 * noise), \n",
    "        mean + std * (mu_pred + 3 * std_dev), \n",
    "        color=aleatoric_color, \n",
    "        alpha=0.5, \n",
    "    )\n",
    "    \n",
    "    plt.legend(loc='upper left')\n",
    "    plt.title(\"Predictive Distribution\")\n",
    "    plt.xlabel('Temperature')\n",
    "    plt.ylabel('Bike Count')\n",
    "    plt.show();\n",
    "\n",
    "interact(\n",
    "    bike_prediction, \n",
    "    noise=ipywidgets.FloatLogSlider(\n",
    "        value=1, min=-2, max=1, continuous_update=False, description=\"Likelihood std dev\", style = {'description_width': 'initial'}),\n",
    "    prior=ipywidgets.FloatLogSlider(value=1e1, min=-2, max=3, continuous_update=False, description=\"Prior Variance\", style = {'description_width': 'initial'}),\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
