{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T18:49:48.412953Z",
     "start_time": "2021-11-11T18:49:47.236565Z"
    }
   },
   "outputs": [],
   "source": [
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib as mpl \n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions import Normal, Categorical\n",
    "\n",
    "from sampling.data import ActiveLearningDataset\n",
    "from sampling.training import Trainer\n",
    "from torch.optim import SGD \n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import ipywidgets\n",
    "from ipywidgets import interact\n",
    "import IPython\n",
    "# If in your browser the figures are not nicely vizualized, change the following line. \n",
    "rcParams['font.size'] = 12\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T18:49:48.540681Z",
     "start_time": "2021-11-11T18:49:48.485352Z"
    }
   },
   "outputs": [],
   "source": [
    "class ExactGP(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood=gpytorch.likelihoods.GaussianLikelihood()):\n",
    "        super().__init__(train_x, train_y, likelihood=likelihood) \n",
    "        self.mean_module = gpytorch.means.ZeroMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward computation of GP.\"\"\"\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "    @property\n",
    "    def output_scale(self):\n",
    "        \"\"\"Get output scale.\"\"\"\n",
    "        return self.covar_module.outputscale\n",
    "    \n",
    "    @property\n",
    "    def length_scale(self):\n",
    "        \"\"\"Get length scale.\"\"\"\n",
    "        return self.covar_module.base_kernel.lengthscale\n",
    "    \n",
    "    @length_scale.setter\n",
    "    def length_scale(self, value):\n",
    "        self.covar_module.base_kernel.lengthscale = value \n",
    "         \n",
    "        \n",
    "class BOpt(nn.Module):\n",
    "    \"\"\"Abstract Bayesian Optimization class. .\"\"\"\n",
    "    def __init__(self, gp, x, beta=2.0):\n",
    "        super().__init__()\n",
    "        self.gp = gp\n",
    "        self.gp.eval()\n",
    "        self.gp.likelihood.eval()\n",
    "        self.x = x\n",
    "        self.update_acquisition_function()\n",
    "    \n",
    "    def update_gp (self, new_inputs, new_targets):\n",
    "        \"\"\"Update GP with new points.\"\"\"\n",
    "        inputs = torch.cat((self.gp.train_inputs[0], new_inputs.unsqueeze(-1)), dim=0)\n",
    "        targets = torch.cat((self.gp.train_targets, new_targets), dim=-1)\n",
    "        self.gp.set_train_data(inputs, targets, strict=False)\n",
    "        self.update_acquisition_function()\n",
    "    \n",
    "    def get_best_value(self):\n",
    "        idx = self.gp.train_targets.argmax()\n",
    "        if len(self.gp.train_targets) == 1:\n",
    "            xmax, ymax = self.gp.train_inputs[idx], self.gp.train_targets[idx]\n",
    "        else:\n",
    "            xmax, ymax = self.gp.train_inputs[0][idx], self.gp.train_targets[idx]\n",
    "        return xmax, ymax \n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    @property \n",
    "    def acquisition_function(self):\n",
    "        return self._acquisition_function \n",
    "    \n",
    "    def forward(self): \n",
    "        \"\"\"Call the algorithm. \"\"\"\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            y = self.acquisition_function\n",
    "            max_id = torch.argmax(y)\n",
    "            next_point = self.x[[[max_id]]]\n",
    "        return next_point\n",
    "    \n",
    "class GPUCB(BOpt):\n",
    "    def __init__(self, gp, x, beta=2.0):\n",
    "        self.beta = beta\n",
    "        super().__init__(gp, x)\n",
    "    \n",
    "    def update_acquisition_function(self):        \n",
    "        pred = self.gp(self.x)\n",
    "        ucb = pred.mean + self.beta * pred.stddev  # Calculate UCB.\n",
    "        self._acquisition_function = ucb \n",
    "    \n",
    "class EpsGreedy(BOpt):\n",
    "    def __init__(self, gp, x, eps=0.1):\n",
    "        self.eps = eps\n",
    "        super().__init__(gp, x)\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        x = self.x + self.eps * torch.randn(self.x.shape)\n",
    "        self._acquisition_function = self.gp(x).mean \n",
    "            \n",
    "class ThompsonSampling(BOpt):\n",
    "    def update_acquisition_function(self):\n",
    "        pred = self.gp(self.x)\n",
    "        self._acquisition_function = pred.sample()\n",
    "        \n",
    "class ProbabilityImprovement(BOpt):\n",
    "    def __init__(self, gp, x, xi=0.01):\n",
    "        self.xi = xi\n",
    "        super().__init__(gp, x)\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        xmax, ymax = self.get_best_value()\n",
    "        out = self.gp(self.x)\n",
    "        dist = Normal(torch.tensor([0.]), torch.tensor([1.]))\n",
    "        Z = (out.mean - ymax - self.xi) / out.stddev\n",
    "        self._acquisition_function = dist.cdf(Z)\n",
    "        \n",
    "class ExpectedImprovement(BOpt):\n",
    "    def __init__(self, gp, x, xi=0.01):\n",
    "        self.xi = xi\n",
    "        super().__init__(gp, x)\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        xmax, ymax = self.get_best_value()\n",
    "        out = self.gp(self.x)\n",
    "        dist = Normal(torch.tensor([0.]), torch.tensor([1.]))\n",
    "        Z = (out.mean - ymax - self.xi) / out.stddev \n",
    "        idx = out.stddev == 0\n",
    "        self._acquisition_function = (out.mean - ymax - self.xi) * dist.cdf(Z) + out.stddev * torch.exp(dist.log_prob(Z))\n",
    "        self._acquisition_function[idx] = 0 \n",
    "        \n",
    "class MaxTraceCov(BOpt):\n",
    "    def update_acquisition_function(self):      \n",
    "        pred = self.gp(self.x)\n",
    "        self._acquisition_function = pred.stddev  # Calculate UCB.\n",
    "\n",
    "class CovSampling(BOpt):\n",
    "    def update_acquisition_function(self):      \n",
    "        pred = self.gp(self.x)\n",
    "        m = Categorical(probs=pred.stddev / pred.stddev.sum())\n",
    "        self._acquisition_function = torch.zeros_like(self.x)\n",
    "        self._acquisition_function[m.sample()] = 1\n",
    "    \n",
    "class UAR(BOpt):\n",
    "    def update_acquisition_function(self): \n",
    "        p = torch.ones_like(self.x)\n",
    "        m = Categorical(probs=p / p.sum())\n",
    "        self._acquisition_function = torch.zeros_like(self.x)\n",
    "        self._acquisition_function[m.sample()] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T18:49:48.650350Z",
     "start_time": "2021-11-11T18:49:48.612848Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_gp(x: torch.Tensor, model: gpytorch.models.GP, num_samples: int, ax) -> None:\n",
    "    \"\"\"Plot 1-D GP.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: points to plot.\n",
    "    model: GP model.\n",
    "    num_samples: number of random samples from gp.\n",
    "    ax: axes where to plot the plot.\n",
    "    \"\"\"\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = model(x)\n",
    "        mean = pred.mean.numpy()\n",
    "        error = 2 * pred.stddev.numpy()\n",
    "        true_values = objective_function.func(x).mean.numpy()\n",
    "\n",
    "    # Plot gp prediction\n",
    "    ax.fill_between(x, mean - error, mean + error, lw=0, alpha=0.2, color='C0')\n",
    "        \n",
    "    # Plot mean\n",
    "    ax.plot(x, mean, color='C0')\n",
    "    \n",
    "    # Plot ground-truth\n",
    "    ax.plot(x, true_values, '--', color='k')\n",
    "    \n",
    "    # Plot data\n",
    "    ax.plot(model.train_inputs[0].numpy(),\n",
    "            model.train_targets.numpy(),\n",
    "            'x', markeredgewidth=2, markersize=10, color='C1')\n",
    "\n",
    "    # Plot samples.\n",
    "    for _ in range(num_samples):\n",
    "        plt.plot(x.numpy(), pred.sample().numpy())\n",
    "    \n",
    "    ax.set_xlim(x[0], x[-1])\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(r\"Input $x$\")\n",
    "    ax.set_ylabel(r\"Objective $f(x)$\")\n",
    "    \n",
    "def plot_acquisition_function(x: torch.Tensor, y: torch.Tensor, ax) -> None:\n",
    "    \"\"\"Plot 1-D Acquisition function.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x: points to plot.\n",
    "    y: values of acquisition_function.\n",
    "    \"\"\"\n",
    "    max_idx = torch.argmax(y)\n",
    "    next_x, next_y = x[max_idx], y[max_idx]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Plot function\n",
    "        ax.plot(x.detach().numpy(), y.detach().numpy())\n",
    "\n",
    "        # Plot maximizer\n",
    "        ax.plot(next_x.detach().numpy(), next_y.detach().numpy(), 'r*', markersize=20, label=\"Next Point\")\n",
    "    \n",
    "    ax.legend(loc=\"lower right\")\n",
    "    ax.set_xlim(x[0], x[-1])\n",
    "    ax.set_ylim(-3, 3)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_xlabel(r\"Input $x$\")\n",
    "    ax.set_ylabel(r\"Acquisition Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T18:49:50.635592Z",
     "start_time": "2021-11-11T18:49:50.004668Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define objective function\n",
    "rcParams['figure.figsize'] = (20, 8)\n",
    "rcParams['font.size'] = 16\n",
    "\n",
    "X = (torch.tensor([-1., 1., 2.5, 4., 6]) - 2.5) / 2\n",
    "Y = torch.tensor([-0.5, 0.3, -0.2, .6, -0.5])\n",
    "\n",
    "NUM_POINTS = 1000\n",
    "x = (torch.linspace(-1, 6, NUM_POINTS) - 2.5) / 2\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "likelihood.noise_covar.noise = 0.01 ** 2\n",
    "\n",
    "\n",
    "class ObjectiveFunction(object):\n",
    "    def __init__(self, gp, noise):\n",
    "        self.func = gp \n",
    "        self.func.eval()\n",
    "        self.noise = noise\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        out = self.func(x)\n",
    "        y = out.mean + self.noise * torch.randn(out.mean.shape)\n",
    "        return y \n",
    "        \n",
    "objective_function = ObjectiveFunction(ExactGP(X, Y, likelihood), noise=0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "# fig.set_size_inches(5.5/2, 1.5)\n",
    "with torch.no_grad():\n",
    "    ax.plot(x, objective_function(x), 'k--')\n",
    "ax.set_xlabel(r\"Input $x$\")\n",
    "ax.set_ylabel(r\"Objective $f(x)$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T19:13:38.606556Z",
     "start_time": "2021-11-11T19:13:38.047556Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_active(name, beta, lengthscale, epsilon, xi, noise):\n",
    "    objective_function.noise = noise \n",
    "    x0 = x[x > -1.4 ][[0]].unsqueeze(-1)\n",
    "    with torch.no_grad():\n",
    "        y0 = objective_function(x0).type(torch.get_default_dtype())\n",
    "    gp = ExactGP(x0, y0, likelihood)\n",
    "    gp.length_scale = lengthscale\n",
    "    gp.likelihood.noise = noise ** 2\n",
    "    if name == \"GP-UCB\":\n",
    "        algorithm =  GPUCB(gp, x, beta=beta)\n",
    "    elif name == \"Thompson\":\n",
    "        algorithm = ThompsonSampling(gp, x)\n",
    "    elif name == \"EpsGreedy\":\n",
    "        algorithm = EpsGreedy(gp, x, eps=epsilon)\n",
    "    elif name == \"EI\":\n",
    "        algorithm = ExpectedImprovement(gp, x, xi=xi)\n",
    "    elif name == \"PI\":\n",
    "        algorithm = ProbabilityImprovement(gp, x, xi=xi)\n",
    "    elif name == \"MaxVar\":\n",
    "        algorithm = MaxTraceCov(gp, x)\n",
    "    elif name == \"CovSampling\":\n",
    "        algorithm = CovSampling(gp, x)\n",
    "    elif name == \"Random\":\n",
    "        algorithm = UAR(gp, x)\n",
    "\n",
    "    ymax = (objective_function.func(x).mean).max()\n",
    "    regret = []\n",
    "    max_var = []\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        max_var.append(algorithm.gp(x).stddev.numpy().max() ** 2)\n",
    "        \n",
    "    def plot_algorithm():\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        \n",
    "        fig, axes = plt.subplots(ncols=2, nrows=2)\n",
    "        \n",
    "        plot_gp(x, algorithm.gp, num_samples=0, ax=axes[0, 0])\n",
    "        plot_acquisition_function(x, algorithm.acquisition_function, ax=axes[0, 1])\n",
    "        axes[1, 0].set_yticklabels([], which=\"both\")\n",
    "        if len(regret):\n",
    "            axes[1, 0].plot(np.cumsum(regret), label=\"Observed Regret\")\n",
    "            axes[1, 0].plot(range(len(regret)), range(len(regret)), 'k--', label=\"Linear\")\n",
    "            axes[1, 0].legend(loc=\"lower right\")\n",
    "            axes[1, 0].set_yticklabels([], minor=True)\n",
    "\n",
    "        \n",
    "        axes[1, 0].set_xticklabels([])\n",
    "        axes[1, 0].set_xlabel(\"Num Episode\")\n",
    "        axes[1, 0].set_ylabel(\"Cum Regret\")\n",
    "        \n",
    "        axes[1, 1].set_yticklabels([], which=\"both\")\n",
    "        if len(regret):\n",
    "            axes[1, 1].plot(max_var)\n",
    "#             axes[1, 1].legend(loc=\"lower right\")\n",
    "            axes[1, 1].set_yticklabels([], minor=True)\n",
    "\n",
    "        \n",
    "        axes[1, 1].set_xticklabels([])\n",
    "        axes[1, 1].set_xlabel(\"Num Episode\")\n",
    "        axes[1, 1].set_ylabel(\"Max Pred. Var.\")\n",
    "        axes[1, 1].set_ylim([0, 1])\n",
    "        plt.show()\n",
    "        button = ipywidgets.Button(description=\"Query New Point\")\n",
    "        button.on_click(lambda b: query_new_point())\n",
    "        display(button)\n",
    "        \n",
    "    def query_new_point():\n",
    "        query_x = algorithm()  # call algorithm to return next query point\n",
    "        query_y = objective_function(query_x).detach()  # evaluate function on point.\n",
    "        regret.append(ymax - query_y)\n",
    "        algorithm.update_gp(query_x, query_y)  # update GP model. \n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            max_var.append(algorithm.gp(x).stddev.numpy().max() ** 2)\n",
    "    \n",
    "        plot_algorithm()\n",
    "        \n",
    "\n",
    "    plot_algorithm()\n",
    "    \n",
    "\n",
    "interact(\n",
    "    run_active,\n",
    "    name=[\"GP-UCB\", \"Thompson\", \"EpsGreedy\", \"EI\", \"PI\", \"MaxVar\", \"CovSampling\", \"Random\"],\n",
    "    beta=ipywidgets.FloatSlider(value=2.0, min=0, max=10, continuous_update=False), \n",
    "    lengthscale=ipywidgets.FloatLogSlider(value=0.6, min=-1, max=2, continuous_update=False),\n",
    "    epsilon=ipywidgets.FloatLogSlider(value=0.1, min=-2, max=0, continuous_update=False),\n",
    "    xi=ipywidgets.FloatSlider(value=0.01, min=0, max=0.1, step=0.01,continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(value=0.01, min=0, max=0.1, step=0.01, continuous_update=False),\n",
    ");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning and Optimizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T19:14:38.958141Z",
     "start_time": "2021-11-11T19:14:38.903878Z"
    }
   },
   "outputs": [],
   "source": [
    "class GPLearner(Trainer):\n",
    "    def __init__(self, train_set, lr, momentum=0.1, *args, **kwargs):\n",
    "        super().__init__(train_set, batch_size=len(train_set), *args, **kwargs)\n",
    "        x, y = train_set.get_observed_data()\n",
    "        self.model = ExactGP(x, y.squeeze())\n",
    "        self.optimizer = SGD(self.model.parameters(), lr=lr, momentum=momentum)\n",
    "        self.mll = ExactMarginalLogLikelihood(self.model.likelihood, self.model)\n",
    "\n",
    "    def fit(self, *args):\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.model(self.model.train_inputs[0])\n",
    "        loss = -self.mll(output, self.model.train_targets.detach())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss\n",
    "\n",
    "    def predict(self, x, num_samples=1):\n",
    "        \"\"\"Output predictive distribution.\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = self.model(x)\n",
    "            epistemic_lower, epistemic_upper = out.confidence_region()\n",
    "            all_lower, all_upper = self.model.likelihood(out).confidence_region()\n",
    "        return out.mean, all_lower, all_upper, out.mean - out.stddev, out.mean + out.stddev\n",
    "    \n",
    "    def update_posterior(self):\n",
    "        x, y = self.train_set.get_observed_data()\n",
    "        self.model.set_train_data(x, y.squeeze(), strict=False)\n",
    "\n",
    "    def sample(self, x):\n",
    "        \"\"\"Sample the function at values x.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            out = self.model(x)\n",
    "            return out.sample()\n",
    "\n",
    "class ActiveLearning(nn.Module):\n",
    "    \"\"\"Abstract Bayesian Optimization class. .\"\"\"\n",
    "    def __init__(self, learner):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.learner = learner\n",
    "        try:\n",
    "            self.learner.model.eval()\n",
    "        except AttributeError:\n",
    "            pass \n",
    "        self.x = self.learner.train_set.x\n",
    "        self.y = self.learner.train_set.y \n",
    "        self.update_acquisition_function()\n",
    "    \n",
    "    def update_model(self, new_idx):\n",
    "        \"\"\"Update GP with new points.\"\"\"\n",
    "        self.learner.train_set.query_index(new_idx)\n",
    "        self.learner.train_loader = DataLoader(\n",
    "            self.learner.train_set, batch_size=len(self.learner.train_set), shuffle=True\n",
    "        )\n",
    "        try:\n",
    "            self.learner.update_posterior()\n",
    "        except AttributeError:\n",
    "            pass \n",
    "        try:\n",
    "            self.learner.model.eval()\n",
    "        except AttributeError:\n",
    "            pass \n",
    "        self.update_acquisition_function()\n",
    "        \n",
    "    def train(self, num_epochs=1):\n",
    "        try:\n",
    "            self.learner.model.train()\n",
    "        except AttributeError:\n",
    "            pass \n",
    "        self.learner.train(num_epochs=num_epochs)\n",
    "        try:\n",
    "            self.learner.model.eval()\n",
    "        except AttributeError:\n",
    "            pass \n",
    "        self.update_acquisition_function()\n",
    "    \n",
    "    def get_best_value(self):\n",
    "        x, y = self.learner.train_set.get_observed_data()\n",
    "        idx = y.argmax()\n",
    "        return x[idx], y[idx]\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        raise NotImplementedError \n",
    "    \n",
    "    @property \n",
    "    def acquisition_function(self):\n",
    "        return self._acquisition_function \n",
    "    \n",
    "    def forward(self): \n",
    "        \"\"\"Call the algorithm. \"\"\"\n",
    "        with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "            y = self.acquisition_function\n",
    "            max_idx = torch.argmax(y)\n",
    "        return max_idx\n",
    "    \n",
    "class AUCB(ActiveLearning):\n",
    "    def __init__(self, learner, beta=2.0):\n",
    "        self.beta = beta\n",
    "        super().__init__(learner)\n",
    "    \n",
    "    def update_acquisition_function(self):        \n",
    "        pred = self.learner.predict_normal(self.x)\n",
    "        ucb = pred.mean + self.beta * pred.stddev  # Calculate UCB.\n",
    "        self._acquisition_function = ucb \n",
    "    \n",
    "class AEpsGreedy(ActiveLearning):\n",
    "    def __init__(self, learner, eps=0.1):\n",
    "        self.eps = eps\n",
    "        super().__init__(learner)\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        x = self.x + self.eps * torch.randn(self.x.shape)\n",
    "        self._acquisition_function = self.learner.predict_normal(x).mean \n",
    "            \n",
    "class AThompsonSampling(ActiveLearning):\n",
    "    def update_acquisition_function(self):\n",
    "        self._acquisition_function = self.learner.sample(self.x)\n",
    "        \n",
    "class AProbabilityImprovement(ActiveLearning):\n",
    "    def __init__(self, learner, xi=0.01):\n",
    "        self.xi = xi\n",
    "        super().__init__(learner)\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        xmax, ymax = self.get_best_value()\n",
    "        out = self.learner.predict_normal(self.x)\n",
    "        dist = Normal(torch.tensor([0.]), torch.tensor([1.]))\n",
    "        Z = (out.mean - ymax - self.xi) / out.stddev\n",
    "        self._acquisition_function = dist.cdf(Z)\n",
    "        \n",
    "class AExpectedImprovement(ActiveLearning):\n",
    "    def __init__(self, learner, xi=0.01):\n",
    "        self.xi = xi\n",
    "        super().__init__(learner)\n",
    "        \n",
    "    def update_acquisition_function(self):\n",
    "        xmax, ymax = self.get_best_value()\n",
    "        out = self.learner.predict_normal(self.x)\n",
    "        dist = Normal(torch.tensor([0.]), torch.tensor([1.]))\n",
    "        Z = (out.mean - ymax - self.xi) / out.stddev \n",
    "        idx = out.stddev == 0\n",
    "        self._acquisition_function = (out.mean - ymax - self.xi) * dist.cdf(Z) + out.stddev * torch.exp(dist.log_prob(Z))\n",
    "        self._acquisition_function[idx] = 0 \n",
    "        \n",
    "class AMaxTraceCov(ActiveLearning):\n",
    "    def update_acquisition_function(self):      \n",
    "        pred = self.learner.predict_normal(self.x)\n",
    "        self._acquisition_function = pred.stddev  # Calculate UCB.\n",
    "\n",
    "class ACovSampling(ActiveLearning):\n",
    "    def update_acquisition_function(self):      \n",
    "        pred = self.learner.predict_normal(self.x)\n",
    "        m = Categorical(probs=pred.stddev / pred.stddev.sum())\n",
    "        self._acquisition_function = torch.zeros_like(self.x)\n",
    "        self._acquisition_function[m.sample()] = 1\n",
    "    \n",
    "class AUAR(ActiveLearning):\n",
    "    def update_acquisition_function(self): \n",
    "        p = torch.ones_like(self.x)\n",
    "        m = Categorical(probs=p / p.sum())\n",
    "        self._acquisition_function = torch.zeros_like(self.x)\n",
    "        self._acquisition_function[m.sample()] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T19:14:39.620204Z",
     "start_time": "2021-11-11T19:14:39.036874Z"
    }
   },
   "outputs": [],
   "source": [
    "from sampling.training import (\n",
    "    SGDLearner, SGLDLearner, MALALearner, SWAGLearner, EnsembleLearner, BootstrapEnsembleLearner, DropoutLearner, BayesBackPropLearner\n",
    ")\n",
    "\n",
    "def plot_predictions(algorithm, ax, num_samples=1, title=\"\"):\n",
    "    ax.plot(algorithm.x.detach(), objective_function.func(algorithm.x).mean.detach(), 'k--', label=\"True Function\")\n",
    "    x, y = algorithm.learner.train_set.get_observed_data()\n",
    "    ax.plot(x.detach(), y.detach(), 'b*', markersize=20, label=\"Data\")\n",
    "    \n",
    "    x = algorithm.x \n",
    "    out = algorithm.learner.predict_normal(x)\n",
    "    ax.plot(x.squeeze().detach(), out.mean.squeeze().detach(), 'b-', label='Mean Prediction')\n",
    "    ax.fill_between(\n",
    "        x.squeeze().detach(), \n",
    "        (out.mean - out.stddev).squeeze().detach(), \n",
    "        (out.mean + out.stddev).squeeze().detach(),\n",
    "        alpha=0.2,\n",
    "        label=\"Confidence Interval\"\n",
    "    )\n",
    "    ax.legend(loc=\"upper left\")\n",
    "    ax.set_ylim([-3, 3])\n",
    "\n",
    "def run_active_lerning(learner_name, alg_name, beta, epsilon, xi, noise):\n",
    "    x = (torch.linspace(-1, 6, NUM_POINTS) - 2.5) / 2\n",
    "    objective_function = ObjectiveFunction(ExactGP(X, Y, likelihood), noise=noise)\n",
    "    dataset = ActiveLearningDataset(x.unsqueeze(-1), objective_function(x).unsqueeze(-1))\n",
    "    dataset.query_index(NUM_POINTS // 2)\n",
    "    dataset.query_index(torch.where(x > -0.8)[0][0].item())\n",
    "    if learner_name == \"GP\":\n",
    "        learner = GPLearner(dataset, lr=0.1)\n",
    "        learner.model.likelihood.noise = 1e-3\n",
    "        learner.model.length_scale = 0.4\n",
    "        num_epochs = 200\n",
    "    elif learner_name == \"SWAG\":\n",
    "        learner = SWAGLearner(\n",
    "            train_set=dataset,\n",
    "            batch_size=len(dataset),\n",
    "            lr=0.005, weight_decay=1e-3, burn_in=5, sub_sample=1,\n",
    "        ) \n",
    "        num_epochs = 100\n",
    "        learner.train(num_epochs)\n",
    "    elif learner_name == \"Ensemble\":\n",
    "        learner = EnsembleLearner(\n",
    "            dataset, batch_size=len(dataset),\n",
    "            num_heads=5, lr=0.001, weight_decay=1e-2,\n",
    "        ) \n",
    "        num_epochs = 40\n",
    "    elif learner_name == \"BootstrapEnsemble\":\n",
    "        learner = BootstrapEnsembleLearner(\n",
    "            dataset, batch_size=len(dataset),\n",
    "            num_heads=5, lr=0.005, weight_decay=1e-2,\n",
    "        ) \n",
    "        num_epochs = 40\n",
    "    elif learner_name == \"Dropout\":\n",
    "        learner = DropoutLearner(\n",
    "            train_set=dataset, batch_size=len(dataset),\n",
    "            dropout_p=0.05, lr=0.005, weight_decay=1e-2,\n",
    "        ) \n",
    "        num_epochs = 40 \n",
    "    elif learner_name == \"SGLD\":\n",
    "        num_epochs=2000\n",
    "        learner = SGLDLearner(\n",
    "            train_set=dataset,\n",
    "            batch_size=len(dataset),\n",
    "            lr=0.02, weight_decay=1e-3, num_iter=num_epochs,\n",
    "            burn_in=5, sub_sample=30, max_size=100,\n",
    "    ) \n",
    "        learner.train(num_epochs)\n",
    "    elif learner_name == \"MALA\":\n",
    "        num_epochs=2000\n",
    "        learner = MALALearner(\n",
    "            train_set=dataset,\n",
    "            batch_size=len(dataset),\n",
    "            lr=0.02, weight_decay=1e-3, num_iter=num_epochs,\n",
    "            burn_in=5, sub_sample=30, max_size=100,\n",
    "    ) \n",
    "        learner.train(num_epochs) \n",
    "    elif learner_name == \"SGD\": \n",
    "        learner = SGDLearner(train_set=dataset, lr=0.001, batch_size=len(dataset), weight_decay=0.01)\n",
    "        num_epochs = 40\n",
    "        \n",
    "    if alg_name == \"UCB\":\n",
    "        algorithm =  AUCB(learner, beta=beta)\n",
    "    elif alg_name == \"Thompson\":\n",
    "        algorithm = AThompsonSampling(learner)\n",
    "    elif alg_name == \"EpsGreedy\":\n",
    "        algorithm = AEpsGreedy(learner, eps=epsilon)\n",
    "    elif alg_name == \"EI\":\n",
    "        algorithm = AExpectedImprovement(learner, xi=xi)\n",
    "    elif alg_name == \"PI\":\n",
    "        algorithm = AProbabilityImprovement(learner, xi=xi)\n",
    "    elif alg_name == \"MaxTrace\":\n",
    "        algorithm = AMaxTraceCov(learner)\n",
    "    elif alg_name == \"CovSampling\":\n",
    "        algorithm = ACovSampling(learner)\n",
    "    elif alg_name == \"Random\":\n",
    "        algorithm = AUAR(learner)\n",
    "\n",
    "    def plot_algorithm():\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        plt.close()\n",
    "        \n",
    "        fig, axes = plt.subplots(ncols=2, nrows=1)\n",
    "        \n",
    "        plot_predictions(algorithm, num_samples=0, ax=axes[0])\n",
    "        plot_acquisition_function(x, algorithm.acquisition_function, ax=axes[1])\n",
    "        \n",
    "        plt.show()\n",
    "        button = ipywidgets.Button(description=\"Query New Point\")\n",
    "        button.on_click(lambda b: query_new_point())\n",
    "        display(button)\n",
    "        \n",
    "        button2 = ipywidgets.Button(description=\"Train\")\n",
    "        button2.on_click(lambda b: train())\n",
    "        display(button2)\n",
    "        \n",
    "        button3 = ipywidgets.Button(description=\"Query Random Point\")\n",
    "        button3.on_click(lambda b: query_random_point())\n",
    "        display(button3)\n",
    "        \n",
    "    def query_new_point():\n",
    "        idx = algorithm().item() # call algorithm to return next query point\n",
    "        algorithm.update_model(idx)  # update GP model. \n",
    "        plot_algorithm()\n",
    "        \n",
    "    def query_random_point():\n",
    "        idx = np.random.choice(NUM_POINTS)  # query a random point\n",
    "        algorithm.update_model(idx)  # update GP model. \n",
    "        plot_algorithm()\n",
    "    \n",
    "    def train():\n",
    "        algorithm.train(num_epochs=num_epochs)\n",
    "        plot_algorithm()\n",
    "        \n",
    "    plot_algorithm()\n",
    "    \n",
    "\n",
    "interact(\n",
    "    run_active_lerning,\n",
    "    learner_name=[\"GP\",  \"BootstrapEnsemble\", \"Dropout\", \"SWAG\", \"MALA\"], #SGLD, Ensemble, SGD\n",
    "    alg_name=[\"UCB\", \"Thompson\", \"EpsGreedy\", \"EI\", \"PI\", \"MaxTrace\", \"CovSampling\", \"Random\"],\n",
    "    beta=ipywidgets.FloatSlider(value=2.0, min=0, max=10, continuous_update=False), \n",
    "    epsilon=ipywidgets.FloatLogSlider(value=0.1, min=-2, max=0, continuous_update=False),\n",
    "    xi=ipywidgets.FloatSlider(value=0.01, min=0, max=0.1, step=0.01,continuous_update=False),\n",
    "    noise=ipywidgets.FloatSlider(value=0.01, min=0, max=0.1, step=0.01, continuous_update=False),\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T17:50:04.578499Z",
     "start_time": "2020-11-05T17:50:04.553993Z"
    }
   },
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-11T19:14:42.221142Z",
     "start_time": "2021-11-11T19:14:41.261481Z"
    }
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = (16, 5)\n",
    "rcParams['font.size'] = 16\n",
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def linear_separable_data(num_positive, num_negative=None, noise=0., offset=1, dim=2):\n",
    "    if num_negative is None:\n",
    "        num_negative = num_positive\n",
    "\n",
    "    x = offset + noise * np.random.randn(num_positive, dim)\n",
    "    y = 1 * np.ones((num_positive,), dtype=np.int)\n",
    "\n",
    "    x = np.concatenate((x, noise * np.random.randn(num_negative, dim)), axis=0)\n",
    "    y = np.concatenate((y, -1 * np.ones((num_negative,), dtype=np.int)), axis=0)\n",
    "\n",
    "    x = np.concatenate((x, np.ones((num_positive + num_negative, 1))), axis=1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def circular_separable_data(num_positive, num_negative=None, noise=0., offset=1, dim=2):\n",
    "    if num_negative is None:\n",
    "        num_negative = num_positive\n",
    "    x = np.random.randn(num_positive, dim)\n",
    "    x = offset * x / np.linalg.norm(x, axis=1, keepdims=True)  # Normalize datapoints to have norm 1.\n",
    "    x += np.random.randn(num_positive, 2) * noise;\n",
    "    y = 1 * np.ones((num_positive,), dtype=np.int)\n",
    "\n",
    "    x = np.concatenate((x, noise * np.random.randn(num_negative, dim)), axis=0)\n",
    "    y = np.concatenate((y, -1 * np.ones((num_negative,), dtype=np.int)), axis=0)\n",
    "    x = np.concatenate((x, np.ones((num_positive + num_negative, 1))), axis=1)\n",
    "\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def get_classification_dataset(dataset, n_samples=200, noise=0.3):\n",
    "    if dataset == 'linear':\n",
    "        X, Y = linear_separable_data(n_samples, noise=noise, dim=2) \n",
    "        Y = (Y + 1) // 2\n",
    "    elif dataset == '2-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=2, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8)\n",
    "    elif dataset == '3-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=3, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8)\n",
    "    elif dataset == '4-blobs':\n",
    "        X, Y = datasets.make_classification(n_classes=4, n_features=2, n_informative=2, n_redundant=0,\n",
    "                                            n_clusters_per_class=1, n_samples=n_samples, random_state=8) \n",
    "    elif dataset == 'circles':\n",
    "        X, Y = datasets.make_circles(n_samples=n_samples, factor=.5, noise=.05)\n",
    "    elif dataset == 'moons':\n",
    "        X, Y = datasets.make_moons(n_samples=n_samples, noise=.05)\n",
    "    elif dataset == 'iris':\n",
    "        X, Y = datasets.load_iris(return_X_y=True)\n",
    "        X = X[:, :2]\n",
    "    elif dataset == 'imbalanced':\n",
    "        X, Y = linear_separable_data(n_samples, noise=noise, dim=2, num_negative=int(n_samples * 0.2))\n",
    "        Y = (Y + 1) // 2\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "queried_set = {}\n",
    "def uncertainty_sampling(dataset, criterion, noise):    \n",
    "    query_button = ipywidgets.Button(description=\"Query new point\")\n",
    "    update_button = ipywidgets.Button(description=\"Update Model\")\n",
    "    restart_button = ipywidgets.Button(description=\"Restart\")\n",
    "    X, Y = get_classification_dataset(dataset, 200, noise=noise)\n",
    "    num_classes = len(np.unique(Y)) - 1\n",
    "    X = X[:, :2]\n",
    "\n",
    "    indexes = np.arange(X.shape[0])\n",
    "    index_set = set([i for i in indexes])\n",
    "\n",
    "\n",
    "    def plot(model, X, Y, queried_set, next_idx=None, display_query=True):\n",
    "        neg_i = np.where(Y == 0)[0]\n",
    "        pos_i = np.where(Y == 1)[0]\n",
    "\n",
    "        queried_idx = [i for i in queried_set]\n",
    "        non_queried_idx = [i for i in index_set.difference(queried_set)]\n",
    "\n",
    "        qX, qY = X[queried_idx], Y[queried_idx]\n",
    "        nX, nY = X[non_queried_idx], Y[non_queried_idx]\n",
    "\n",
    "        # Model prediction contours.\n",
    "        x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "        y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "        h = .02 \n",
    "        xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "        xy = np.c_[xx.ravel(), yy.ravel()]\n",
    "        probs = model.predict_proba(xy)\n",
    "        probs.sort(axis=1)\n",
    "        P = model.predict_proba(xy).max(axis=1).reshape(xx.shape)\n",
    "        np.testing.assert_allclose(probs[:, -1].reshape(xx.shape), P)\n",
    "        confidence_margin = (probs[:, -1] - probs[:, -2]).reshape(xx.shape)\n",
    "        confidence_ratio = (probs[:, -1] / probs[:, -2]).reshape(xx.shape)\n",
    "        C = model.predict(xy).reshape(xx.shape)\n",
    "        H = -(model.predict_proba(xy) * model.predict_log_proba(xy)).sum(axis=1).reshape(xx.shape)\n",
    "        \n",
    "        # PLOTS\n",
    "        fig, axes = plt.subplots(1, 2)\n",
    "        axes[0].set_title('Classification Boundary')\n",
    "        axes[0].contourf(xx, yy, C, cmap=plt.cm.jet, alpha=0.5, vmin=0, vmax=num_classes)\n",
    "        \n",
    "        if criterion == 'max-entropy':\n",
    "            axes[1].set_title('Entropy')\n",
    "            cf = axes[1].contourf(xx, yy, H, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "            m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "            m.set_array(H)\n",
    "            cbar = plt.colorbar(m, ax=axes[1])  \n",
    "            cbar.set_label('Predicted Entropy', rotation=270, labelpad=20)\n",
    "            \n",
    "        elif criterion == 'min-probability':\n",
    "            axes[1].set_title('Probability')\n",
    "            cf = axes[1].contourf(xx, yy, P, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "            m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "            m.set_array(P)\n",
    "            cbar = plt.colorbar(m, ax=axes[1])  \n",
    "            cbar.set_label('Predicted Probability', rotation=270, labelpad=20)\n",
    "            \n",
    "        elif criterion == 'confidence-margin':\n",
    "            axes[1].set_title('Confidence Margin')\n",
    "            cf = axes[1].contourf(xx, yy, confidence_margin, cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "            m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "            m.set_array(confidence_margin)\n",
    "            cbar = plt.colorbar(m, ax=axes[1])  \n",
    "            cbar.set_label('Confidence Margin', rotation=270, labelpad=20)\n",
    "        \n",
    "        elif criterion == 'confidence-ratio':\n",
    "            axes[1].set_title('Confidence Ratio')\n",
    "            cf = axes[1].contourf(xx, yy, np.log(confidence_ratio), cmap=plt.cm.cividis_r, alpha=0.5)\n",
    "            m = plt.cm.ScalarMappable(cmap=plt.cm.cividis_r)\n",
    "            m.set_array(np.log(confidence_ratio))\n",
    "            cbar = plt.colorbar(m, ax=axes[1])  \n",
    "            cbar.set_label('Log Ratio', rotation=270, labelpad=20)\n",
    "            \n",
    "        # Plot also the training points\n",
    "        for ax in axes:\n",
    "            ax.scatter(qX[:, 0], qX[:, 1], c=qY, marker='o', s=200, cmap=plt.cm.jet, vmin=0, vmax=num_classes)\n",
    "            ax.scatter(nX[:, 0], nX[:, 1], c=nY, marker='o', alpha=0.3, s=20, cmap=plt.cm.jet, vmin=0, vmax=num_classes)\n",
    "            \n",
    "            if next_idx is not None:\n",
    "                ax.scatter(X[[next_idx], 0], X[[next_idx], 1], c=Y[[next_idx]], s=400, marker='*',\n",
    "                           cmap=plt.cm.jet, vmin=0, vmax=num_classes)\n",
    "            \n",
    "            ax.set_xlim(xx.min(), xx.max())\n",
    "            ax.set_ylim(yy.min(), yy.max())\n",
    "            ax.set_xticks(())\n",
    "            ax.set_yticks(())\n",
    "\n",
    "        \n",
    "        IPython.display.clear_output(wait=True)\n",
    "        IPython.display.display(plt.gcf())\n",
    "        plt.close()\n",
    "\n",
    "        if display_query:\n",
    "            display(query_button)\n",
    "        else:\n",
    "            display(update_button)\n",
    "        display(restart_button)\n",
    "\n",
    "    def update_model(b):\n",
    "        global queried_set, model\n",
    "\n",
    "        queried_idx = [i for i in queried_set]\n",
    "        model = LogisticRegression(C=10).fit(X[queried_idx], Y[queried_idx])\n",
    "\n",
    "        plot(model, X, Y, queried_set, next_idx=None, display_query=True)\n",
    "    \n",
    "    def restart(b):\n",
    "        global queried_set\n",
    "        queried_set = set()\n",
    "        classes = np.unique(Y)\n",
    "        for c in classes:\n",
    "            i = np.random.choice(np.where(Y == c)[0])\n",
    "            queried_set.add(i)\n",
    "        update_model(None)\n",
    "\n",
    "    def append_point(b):\n",
    "        global queried_set, model\n",
    "        \n",
    "        query_points = X\n",
    "        probs = model.predict_proba(X)\n",
    "        probs.sort(axis=1) \n",
    "        H = model.predict_log_proba(X) * model.predict_proba(X)\n",
    "        H = H.sum(axis=1)\n",
    "        confidence_margin = probs[:, -1] - probs[:, -2]\n",
    "        confidence_ratio = probs[:, -1] / probs[:, -2]\n",
    "        queried_idx = [i for i in queried_set]\n",
    "        probs[queried_idx] = float('Inf')\n",
    "        H[queried_idx] = float('Inf')\n",
    "        confidence_margin[queried_idx] = float('Inf')\n",
    "        confidence_ratio[queried_idx] = float('Inf')\n",
    "        \n",
    "        if criterion == 'max-entropy':\n",
    "            i = np.argmin(H)\n",
    "        elif criterion == 'min-probability':\n",
    "            i = np.argmin(probs[:, -1])\n",
    "        elif criterion == \"confidence-margin\":\n",
    "            i = np.argmin(confidence_margin)\n",
    "        elif criterion == \"confidence-ratio\":\n",
    "            i = np.argmin(confidence_ratio)\n",
    "            \n",
    "        plot(model, X, Y, queried_set,  i, display_query=False)\n",
    "        queried_set.add(i)\n",
    "\n",
    "    query_button.on_click(append_point)\n",
    "    update_button.on_click(update_model)\n",
    "    restart_button.on_click(restart)\n",
    "\n",
    "    restart(None);\n",
    "    \n",
    "interact(uncertainty_sampling, \n",
    "         dataset=['linear', 'imbalanced', '2-blobs', '3-blobs', '4-blobs', 'iris', 'circles', 'moons'], \n",
    "         criterion=['min-probability', 'max-entropy', 'confidence-margin', 'confidence-ratio'],\n",
    "         noise=ipywidgets.FloatSlider(value=0.25, min=0, max=1, step=0.01, readout_format='.2f',\n",
    "                                      continuous_update=False));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
